{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f37b9ea",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39a0af",
   "metadata": {},
   "source": [
    "### Steps Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eab0da",
   "metadata": {},
   "source": [
    "Dataset Organization: Directory structure\n",
    "\n",
    "```plaintext\n",
    "data/raw/\n",
    "├── Calc-Test_P_00038_LEFT_CC/\n",
    "│   └── .../1-1.dcm         (full mammogram image)\n",
    "├── Calc-Test_P_00038_LEFT_CC_1/\n",
    "│   └── .../1-1.dcm, 1-2.dcm (ROI mask images)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38025825",
   "metadata": {},
   "source": [
    "1. Scan DICOM Files: \n",
    "   - Recursively collects full mammogram and ROI mask `.dcm` files from `data/raw/`.\n",
    "   - Input: data/raw/\n",
    "   - Output: List of file paths (full_mammo, roi_masks)\n",
    "2. Extract Metadata: \n",
    "   - Parses abnormality type, patient ID, laterality, and view from folder names.\n",
    "   - Input: file paths\n",
    "   - Output: structured dictionary per DICOM\n",
    "3. Pair Images and Masks: \n",
    "   - Matches each full mammogram to its corresponding ROI masks.\n",
    "   - Input: lists from Step 1\n",
    "   - Output: paired image-mask metadata\n",
    "4. Consolidate Clinical Metadata: \n",
    "   - Merges multiple CSVs (calc and mass cases) into one DataFrame.\n",
    "   - Input: data/metadata/ CSV files\n",
    "   - Output: consolidated clinical metadata\n",
    "5. Merge Metadata: \n",
    "   - Combines image-mask metadata with clinical metadata, exports to a CSV.\n",
    "   - Input: outputs of Step 3 and Step 4\n",
    "   - Output: cbis_ddsm_metadata_full.csv\n",
    "6. Build TensorFlow Dataset: \n",
    "   - Creates a `tf.data.Dataset` with preprocessing for model training.\n",
    "   - Input: final metadata CSV\n",
    "   - Output: TensorFlow-ready dataset (train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa0065",
   "metadata": {},
   "source": [
    "### Scan DICOM Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff71383",
   "metadata": {},
   "source": [
    "- Recursively scan all .dcm files from `base_dir`.\n",
    "- Separate into full mammogram images and ROI mask images.\n",
    "\n",
    "- Args:\n",
    "  - base_dir (str): Root directory containing DICOM folders (e.g., \"data/raw/\").\n",
    "\n",
    "- Returns:\n",
    "  - dict: {\n",
    "    - \"full_mammo\": [list of full mammogram DICOM paths],\n",
    "    - \"roi_masks\": [list of ROI mask DICOM paths] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e234b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "def scan_dicom_files(base_dir: str) -> Dict[str, List[str]]:\n",
    "    base_path = Path(base_dir)\n",
    "    dicom_files = {\n",
    "        \"full_mammo\": [],\n",
    "        \"roi_masks\": [],\n",
    "    }\n",
    "\n",
    "    if not base_path.exists():\n",
    "        raise ValueError(f\"Provided base directory does not exist: {base_dir}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".dcm\"):\n",
    "                file_path = Path(root) / file\n",
    "                lower_root = str(root).lower()\n",
    "\n",
    "                if \"full mammogram images\" in lower_root:\n",
    "                    dicom_files[\"full_mammo\"].append(str(file_path))\n",
    "                elif \"roi mask images\" in lower_root:\n",
    "                    dicom_files[\"roi_masks\"].append(str(file_path))\n",
    "\n",
    "    print(f\"[INFO] Found {len(dicom_files['full_mammo'])} full mammogram DICOMs.\")\n",
    "    print(f\"[INFO] Found {len(dicom_files['roi_masks'])} ROI mask DICOMs.\")\n",
    "\n",
    "    return dicom_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2049ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 3103 full mammogram DICOMs.\n",
      "[INFO] Found 7026 ROI mask DICOMs.\n",
      "\n",
      "Example full mammogram paths:\n",
      "../data/raw/Mass-Test_P_00066_LEFT_CC/10-04-2016-DDSM-NA-12982/1.000000-full mammogram images-25433/1-1.dcm\n",
      "../data/raw/Calc-Test_P_02176_RIGHT_MLO/08-29-2017-DDSM-NA-33174/1.000000-full mammogram images-82696/1-1.dcm\n",
      "../data/raw/Calc-Training_P_00418_LEFT_CC/08-07-2016-DDSM-NA-95820/1.000000-full mammogram images-43865/1-1.dcm\n",
      "\n",
      "Example ROI mask paths:\n",
      "../data/raw/Calc-Training_P_01205_RIGHT_MLO_1/09-06-2017-DDSM-NA-99604/1.000000-ROI mask images-30743/1-1.dcm\n",
      "../data/raw/Calc-Training_P_01205_RIGHT_MLO_1/09-06-2017-DDSM-NA-99604/1.000000-ROI mask images-30743/1-2.dcm\n",
      "../data/raw/Calc-Training_P_00778_LEFT_CC_1/09-06-2017-DDSM-NA-38576/1.000000-ROI mask images-63875/1-1.dcm\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../data/raw/\"\n",
    "dicom_paths = scan_dicom_files(base_dir)\n",
    "\n",
    "# Access full mammograms and ROI masks\n",
    "full_mammo_files = dicom_paths[\"full_mammo\"]\n",
    "roi_mask_files = dicom_paths[\"roi_masks\"]\n",
    "\n",
    "# Preview first few rows\n",
    "print(\"\\nExample full mammogram paths:\")\n",
    "for path in full_mammo_files[:3]:\n",
    "    print(path)\n",
    "\n",
    "print(\"\\nExample ROI mask paths:\")\n",
    "for path in roi_mask_files[:3]:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a13a9",
   "metadata": {},
   "source": [
    "### Extract Metadata from File Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5ed3f",
   "metadata": {},
   "source": [
    "- Extracts abnormality type, patient ID, laterality, and view from CBIS-DDSM DICOM file path.\n",
    "\n",
    "- Args:\n",
    "  - path (str or Path): Full path to a DICOM file.\n",
    "\n",
    "- Returns:\n",
    "  - dict: { \n",
    "      - \"abnormality_type\": \"Calc\" or \"Mass\",\n",
    "      - \"patient_id\": \"00038\",\n",
    "      - \"laterality\": \"LEFT\" or \"RIGHT\",\n",
    "      - \"view\": \"CC\" or \"MLO\",\n",
    "      - \"path\": full file path (str) }\n",
    "  - If parsing fails, returns None for the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict\n",
    "\n",
    "# Extract abnormality type, patient ID, laterality, view from any DICOM path \n",
    "# by searching path parts for the folder name.\n",
    "def extract_metadata_from_path(path: Union[str, Path]) -> Dict[str, Union[str, None]]:\n",
    "    path = Path(path)\n",
    "\n",
    "    # Try to find the folder matching \"Calc-...\" or \"Mass-...\" with pattern\n",
    "    for part in path.parts:\n",
    "        pattern = r\"^(Calc|Mass)-(Test|Training)_P_(\\d+)_([A-Z]+)_(CC|MLO)\"\n",
    "        match = re.match(pattern, part)\n",
    "        if match:\n",
    "            abnormality_type, dataset_split, patient_id, laterality, view = match.groups()\n",
    "            return {\n",
    "                \"abnormality_type\": abnormality_type,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"laterality\": laterality,\n",
    "                \"view\": view,\n",
    "                \"path\": str(path)\n",
    "            }\n",
    "    \n",
    "    # If no match found\n",
    "    return {\n",
    "        \"abnormality_type\": None,\n",
    "        \"patient_id\": None,\n",
    "        \"laterality\": None,\n",
    "        \"view\": None,\n",
    "        \"path\": str(path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea629a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Metadata:\n",
      "abnormality_type: Calc\n",
      "patient_id: 00038\n",
      "laterality: LEFT\n",
      "view: CC\n",
      "path: ../data/raw/Calc-Test_P_00038_LEFT_CC/08-29-2017-DDSM-NA-96009/1.000000-full mammogram images-63992/1-1.dcm\n"
     ]
    }
   ],
   "source": [
    "dicom_path = \"../data/raw/Calc-Test_P_00038_LEFT_CC/08-29-2017-DDSM-NA-96009/1.000000-full mammogram images-63992/1-1.dcm\"\n",
    "\n",
    "metadata = extract_metadata_from_path(dicom_path)\n",
    "\n",
    "print(\"Extracted Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e38f194",
   "metadata": {},
   "source": [
    "### Pair Images and Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6756e",
   "metadata": {},
   "source": [
    "- Pairs full mammogram images with corresponding ROI mask images based on naming convention.\n",
    "\n",
    "- Args:\n",
    "  - full_mammo_paths (List[str]): List of full mammogram DICOM paths.\n",
    "  - roi_mask_paths (List[str]): List of ROI mask DICOM paths.\n",
    "\n",
    "- Returns:\n",
    "  - List[Dict[str, str]]: List of dictionaries with keys:\n",
    "    - abnormality_type\n",
    "    - patient_id\n",
    "    - laterality\n",
    "    - view\n",
    "    - image_path\n",
    "    - mask_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Extracts a base key like 'Calc-Test_P_00038_LEFT_CC' or 'Mass-Test_P_00123_RIGHT_MLO'\n",
    "# from a DICOM file path, ignoring mask/image folder suffixes.\n",
    "# Args: path (str): Full DICOM file path.\n",
    "# Returns: str: Standardized base key for pairing (patient_id + view + laterality).\n",
    "def get_base_key(path: str) -> str:\n",
    "    path = Path(path)\n",
    "\n",
    "    # Search for the correct part\n",
    "    for part in path.parts:\n",
    "        pattern = r\"^(Calc|Mass)-(Test|Training)_P_(\\d+)_([A-Z]+)_(CC|MLO)\"\n",
    "        match = re.match(pattern, part)\n",
    "        if match:\n",
    "            abnormality_type, dataset_split, patient_id, laterality, view = match.groups()\n",
    "            return f\"{abnormality_type}-Test_P_{patient_id}_{laterality}_{view}\"\n",
    "    \n",
    "    # If not found, fallback\n",
    "    return None\n",
    "\n",
    "\n",
    "def pair_images_and_masks(full_mammo_paths, roi_mask_paths):\n",
    "    grouped_masks = defaultdict(list)\n",
    "    \n",
    "    for mask_path in roi_mask_paths:\n",
    "        base_key = get_base_key(mask_path)\n",
    "        grouped_masks[base_key].append(str(mask_path))\n",
    "\n",
    "    paired_records = []\n",
    "\n",
    "    for image_path in full_mammo_paths:\n",
    "        metadata = extract_metadata_from_path(image_path)\n",
    "        base_key = get_base_key(image_path)\n",
    "        mask_list = grouped_masks.get(base_key, [])\n",
    "\n",
    "        record = metadata.copy()\n",
    "        record[\"image_path\"] = str(image_path)\n",
    "        record[\"mask_paths\"] = mask_list  # <== LIST of mask paths!\n",
    "        paired_records.append(record)\n",
    "\n",
    "    return paired_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 3103 full mammogram DICOMs.\n",
      "[INFO] Found 7026 ROI mask DICOMs.\n",
      "  abnormality_type patient_id laterality view  \\\n",
      "0             Mass      00066       LEFT   CC   \n",
      "1             Calc      02176      RIGHT  MLO   \n",
      "2             Calc      00418       LEFT   CC   \n",
      "3             Mass      01307      RIGHT  MLO   \n",
      "4             Mass      00488       LEFT   CC   \n",
      "\n",
      "                                                path  \\\n",
      "0  ../data/raw/Mass-Test_P_00066_LEFT_CC/10-04-20...   \n",
      "1  ../data/raw/Calc-Test_P_02176_RIGHT_MLO/08-29-...   \n",
      "2  ../data/raw/Calc-Training_P_00418_LEFT_CC/08-0...   \n",
      "3  ../data/raw/Mass-Test_P_01307_RIGHT_MLO/10-04-...   \n",
      "4  ../data/raw/Mass-Training_P_00488_LEFT_CC/07-2...   \n",
      "\n",
      "                                          image_path  \\\n",
      "0  ../data/raw/Mass-Test_P_00066_LEFT_CC/10-04-20...   \n",
      "1  ../data/raw/Calc-Test_P_02176_RIGHT_MLO/08-29-...   \n",
      "2  ../data/raw/Calc-Training_P_00418_LEFT_CC/08-0...   \n",
      "3  ../data/raw/Mass-Test_P_01307_RIGHT_MLO/10-04-...   \n",
      "4  ../data/raw/Mass-Training_P_00488_LEFT_CC/07-2...   \n",
      "\n",
      "                                          mask_paths  \n",
      "0  [../data/raw/Mass-Test_P_00066_LEFT_CC_1/10-04...  \n",
      "1  [../data/raw/Calc-Test_P_02176_RIGHT_MLO_1/08-...  \n",
      "2  [../data/raw/Calc-Training_P_00418_LEFT_CC_1/0...  \n",
      "3  [../data/raw/Mass-Test_P_01307_RIGHT_MLO_1/10-...  \n",
      "4  [../data/raw/Mass-Training_P_00488_LEFT_CC_1/0...  \n"
     ]
    }
   ],
   "source": [
    "# from scan_dicom_files import scan_dicom_files\n",
    "dicom_files = scan_dicom_files(\"../data/raw/\")\n",
    "\n",
    "full_mammo_paths = dicom_files[\"full_mammo\"]\n",
    "roi_mask_paths = dicom_files[\"roi_masks\"]\n",
    "\n",
    "paired_metadata = pair_images_and_masks(full_mammo_paths, roi_mask_paths)\n",
    "\n",
    "# Convert to DataFrame and preview\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(paired_metadata)\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df.to_csv(\"../data/processed/cbis_ddsm_metadata_paired.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e220975",
   "metadata": {},
   "source": [
    "### Consolidate Clinical Metadata CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88faec",
   "metadata": {},
   "source": [
    "The four separately provided CBIS-DDSM metadata CSV files contain critical clinical information—such as BI-RADS category, pathology, assessment, and lesion subtlety—that are not embedded in the DICOM files or their folder names.\n",
    "\n",
    "- data/metadata/calc_case_description_test_set.csv\n",
    "- data/metadata/calc_case_description_train_set.csv\n",
    "- data/metadata/mass_case_description_test_set.csv\n",
    "- data/metadata/mass_case_description_train_set.csv\n",
    "\n",
    "Each rows contains the following columns:\n",
    "\n",
    "- Patient ID: `P_00038` — Unique patient identifier.\n",
    "- Breast Side: `LEFT` — Left breast.\n",
    "- Image View: `CC` or `MLO` — Standard cranio-caudal or mediolateral oblique view used in mammography.\n",
    "- Breast Density: `2`\n",
    "- Abnormality ID: `1`\n",
    "- Abnormality Type: `calcification` — Specifically dealing with microcalcifications.\n",
    "- Calcification Type: `PUNCTATE-PLEOMORPHIC` — Mixed types, suggesting variable morphology.\n",
    "- Calcification Distribution: `CLUSTERED` — Clustered microcalcifications, often suspicious.\n",
    "- Assessment: `4` — BI-RADS 4, suspicious abnormality; biopsy usually recommended.\n",
    "- Pathology: `BENIGN` — Biopsy/pathology confirmed the finding as benign.\n",
    "- Subtlety: `2` — Fairly subtle (1 = very subtle, 5 = very obvious).\n",
    "- Image Files:\n",
    "  - Original Image Path: Full mammogram DICOM.\n",
    "    Calc-Test_P_00038_LEFT_CC/1.3.6.1.4.1.9590.100.1.2.85935434310203356712688695661986996009/1.3.6.1.4.1.9590.100.1.2.374115997511889073021386151921807063992/000000.dcm\n",
    "  - Cropped Image Path: Focused region where calcifications are.\n",
    "    Calc-Test_P_00038_LEFT_CC_1/1.3.6.1.4.1.9590.100.1.2.161465562211359959230647609981488894942/1.3.6.1.4.1.9590.100.1.2.419081637812053404913157930753972718515/000001.dcm\n",
    "  - ROI Mask Path: Binary mask of the calcifications (region of interest).\n",
    "    Calc-Test_P_00038_LEFT_CC_1/1.3.6.1.4.1.9590.100.1.2.161465562211359959230647609981488894942/1.3.6.1.4.1.9590.100.1.2.419081637812053404913157930753972718515/000000.dcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46957c",
   "metadata": {},
   "source": [
    "The following script reads and merges the metadata CSV files, cleans weird paths, adds missing fields where needed and exports everything into a properly formatted metadata_master.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d880bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master metadata CSV created: ../data/metadata/metadata_master.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Input CSV files\n",
    "input_files = [\n",
    "    '../data/metadata/calc_case_description_test_set.csv',\n",
    "    '../data/metadata/calc_case_description_train_set.csv',\n",
    "    '../data/metadata/mass_case_description_test_set.csv',\n",
    "    '../data/metadata/mass_case_description_train_set.csv'\n",
    "]\n",
    "\n",
    "# Read CSVs by properly handling malformed newlines and all the data cleanly kept.\n",
    "dfs = []\n",
    "for file in input_files:\n",
    "    df = pd.read_csv(\n",
    "        file,\n",
    "        engine=\"python\",        # Use Python engine to properly handle malformed newlines\n",
    "        quoting=csv.QUOTE_MINIMAL, # Respect quotes\n",
    "        skip_blank_lines=True   # Optional: Skip totally blank lines\n",
    "    )\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all together\n",
    "metadata = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "metadata = metadata.rename(columns={\n",
    "    'patient_id': 'patient_id',\n",
    "    'left or right breast': 'side',\n",
    "    'image view': 'view',\n",
    "    'abnormality id': 'abnormality_id',\n",
    "    'abnormality type': 'abnormality_type',\n",
    "    'calc type': 'calc_type',\n",
    "    'calc distribution': 'distribution',\n",
    "    'mass shape': 'mass_shape',\n",
    "    'mass margins': 'mass_margins',\n",
    "    'breast density': 'breast_density',\n",
    "    'assessment': 'assessment',\n",
    "    'pathology': 'pathology',\n",
    "    'subtlety': 'subtlety',\n",
    "    'image file path': 'full_mammo_path',\n",
    "    'cropped image file path': 'cropped_roi_path',\n",
    "    'ROI mask file path': 'roi_mask_path'\n",
    "})\n",
    "\n",
    "# Add missing columns if needed\n",
    "for col in ['calc_type', 'distribution', 'mass_shape', 'mass_margins']:\n",
    "    if col not in metadata.columns:\n",
    "        metadata[col] = pd.NA\n",
    "\n",
    "# Normalize file paths\n",
    "def fix_path(path):\n",
    "    if pd.isna(path):\n",
    "        return None\n",
    "    # Remove a newline character embedded inside the \"cropped image file path\" field.\n",
    "    path = path.strip().replace('\\\\', '/').replace('\\\"', '')\n",
    "    parts = Path(path).parts\n",
    "    if len(parts) < 4:\n",
    "        return path\n",
    "    parent_folder = parts[0]\n",
    "    subfolder = parts[1]\n",
    "    file_name = parts[-1]\n",
    "    return f'raw/{parent_folder}/{subfolder}/{file_name}'\n",
    "\n",
    "# Apply path fixing\n",
    "for col in ['full_mammo_path', 'cropped_roi_path', 'roi_mask_path']:\n",
    "    metadata[col] = metadata[col].apply(fix_path)\n",
    "\n",
    "# Select final columns\n",
    "final_cols = [\n",
    "    'patient_id', 'breast_density', 'side', 'view', 'abnormality_id',\n",
    "    'abnormality_type', 'calc_type', 'distribution', 'mass_shape', 'mass_margins',\n",
    "    'assessment', 'pathology', 'subtlety',\n",
    "    'full_mammo_path', 'cropped_roi_path', 'roi_mask_path'\n",
    "]\n",
    "metadata = metadata[final_cols]\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../data/metadata/metadata_master.csv'\n",
    "metadata.to_csv(output_path, index=False)\n",
    "print(f'Master metadata CSV created: {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5585c02",
   "metadata": {},
   "source": [
    "### Merge Image–Mask Metadata and Clinical Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df934b",
   "metadata": {},
   "source": [
    "- Merge paired image–mask metadata with clinical metadata based on patient_id, view, laterality, and abnormality_id.\n",
    "\n",
    "- Args:\n",
    "  - paired_metadata_path (str): Path to the paired image-mask metadata CSV.\n",
    "  - clinical_metadata_path (str): Path to the consolidated clinical metadata CSV.\n",
    "  - output_path (str): Path to save the merged master metadata CSV.\n",
    "\n",
    "- Returns:\n",
    "  - pd.DataFrame: Final merged metadata DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ee53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_metadata(\n",
    "    paired_metadata_path: str,\n",
    "    clinical_metadata_path: str,\n",
    "    output_path: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Load CSVs\n",
    "    paired_df = pd.read_csv(paired_metadata_path)\n",
    "    clinical_df = pd.read_csv(clinical_metadata_path)\n",
    "\n",
    "    # Normalize column names if needed\n",
    "    paired_df = paired_df.rename(columns={\n",
    "        'side': 'laterality'  # If needed (depending on how it was named)\n",
    "    })\n",
    "\n",
    "    # Standardize patient_id\n",
    "    # Remove 'P_' prefix from clinical metadata patient IDs\n",
    "    clinical_df['patient_id'] = clinical_df['patient_id'].astype(str).str.replace('P_', '').str.zfill(5)\n",
    "    # Paired metadata: ensure patient_id is 5-digit zero-padded string\n",
    "    paired_df['patient_id'] = paired_df['patient_id'].astype(str).str.zfill(5)\n",
    "\n",
    "    # Ensure consistent casing for joining keys\n",
    "    paired_df['laterality'] = paired_df['laterality'].astype(str).str.upper()\n",
    "    paired_df['view'] = paired_df['view'].astype(str).str.upper()\n",
    "    clinical_df['side'] = clinical_df['side'].astype(str).str.upper()\n",
    "    clinical_df['view'] = clinical_df['view'].astype(str).str.upper()\n",
    "\n",
    "    # Merge: LEFT JOIN, keep all paired image/mask metadata\n",
    "    merged = pd.merge(\n",
    "        paired_df,\n",
    "        clinical_df,\n",
    "        how=\"left\",\n",
    "        left_on=[\"patient_id\", \"laterality\", \"view\"],\n",
    "        right_on=[\"patient_id\", \"side\", \"view\"]\n",
    "    )\n",
    "\n",
    "    # Drop duplicate columns (like \"side\" from clinical metadata)\n",
    "    if \"side\" in merged.columns:\n",
    "        merged = merged.drop(columns=[\"side\"])\n",
    "\n",
    "    # Add binary label from pathology if present\n",
    "    if 'pathology' in merged.columns:\n",
    "        merged['label'] = merged['pathology'].map({'MALIGNANT': 1, 'BENIGN': 0}).astype('float32')\n",
    "\n",
    "    # Save merged DataFrame\n",
    "    output_dir = Path(output_path).parent\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    merged.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"[INFO] Master merged metadata saved at: {output_path}\")\n",
    "    print(f\"[INFO] Merged metadata shape: {merged.shape}\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac9addf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Master merged metadata saved at: ../data/processed/cbis_ddsm_metadata_full.csv\n",
      "[INFO] Merged metadata shape: (3751, 22)\n",
      "  abnormality_type_x patient_id laterality view  \\\n",
      "0               Mass      00066       LEFT   CC   \n",
      "1               Calc      02176      RIGHT  MLO   \n",
      "2               Calc      00418       LEFT   CC   \n",
      "3               Mass      01307      RIGHT  MLO   \n",
      "4               Mass      00488       LEFT   CC   \n",
      "\n",
      "                                                path  \\\n",
      "0  ../data/raw/Mass-Test_P_00066_LEFT_CC/10-04-20...   \n",
      "1  ../data/raw/Calc-Test_P_02176_RIGHT_MLO/08-29-...   \n",
      "2  ../data/raw/Calc-Training_P_00418_LEFT_CC/08-0...   \n",
      "3  ../data/raw/Mass-Test_P_01307_RIGHT_MLO/10-04-...   \n",
      "4  ../data/raw/Mass-Training_P_00488_LEFT_CC/07-2...   \n",
      "\n",
      "                                          image_path  \\\n",
      "0  ../data/raw/Mass-Test_P_00066_LEFT_CC/10-04-20...   \n",
      "1  ../data/raw/Calc-Test_P_02176_RIGHT_MLO/08-29-...   \n",
      "2  ../data/raw/Calc-Training_P_00418_LEFT_CC/08-0...   \n",
      "3  ../data/raw/Mass-Test_P_01307_RIGHT_MLO/10-04-...   \n",
      "4  ../data/raw/Mass-Training_P_00488_LEFT_CC/07-2...   \n",
      "\n",
      "                                          mask_paths  breast_density  \\\n",
      "0  ['../data/raw/Mass-Test_P_00066_LEFT_CC_1/10-0...             NaN   \n",
      "1  ['../data/raw/Calc-Test_P_02176_RIGHT_MLO_1/08...             4.0   \n",
      "2  ['../data/raw/Calc-Training_P_00418_LEFT_CC_1/...             3.0   \n",
      "3  ['../data/raw/Mass-Test_P_01307_RIGHT_MLO_1/10...             NaN   \n",
      "4  ['../data/raw/Mass-Training_P_00488_LEFT_CC_1/...             NaN   \n",
      "\n",
      "   breast_density.1  abnormality_id  ... distribution mass_shape  \\\n",
      "0               4.0               1  ...          NaN  IRREGULAR   \n",
      "1               NaN               1  ...     REGIONAL        NaN   \n",
      "2               NaN               1  ...    CLUSTERED        NaN   \n",
      "3               3.0               1  ...          NaN      ROUND   \n",
      "4               4.0               1  ...          NaN  IRREGULAR   \n",
      "\n",
      "             mass_margins assessment  pathology  subtlety  \\\n",
      "0              SPICULATED          4  MALIGNANT         3   \n",
      "1                     NaN          5  MALIGNANT         5   \n",
      "2                     NaN          4  MALIGNANT         2   \n",
      "3           CIRCUMSCRIBED          3     BENIGN         5   \n",
      "4  CIRCUMSCRIBED-OBSCURED          4     BENIGN         3   \n",
      "\n",
      "                                     full_mammo_path  \\\n",
      "0  raw/Mass-Test_P_00066_LEFT_CC/1.3.6.1.4.1.9590...   \n",
      "1  raw/Calc-Test_P_02176_RIGHT_MLO/1.3.6.1.4.1.95...   \n",
      "2  raw/Calc-Training_P_00418_LEFT_CC/1.3.6.1.4.1....   \n",
      "3  raw/Mass-Test_P_01307_RIGHT_MLO/1.3.6.1.4.1.95...   \n",
      "4  raw/Mass-Training_P_00488_LEFT_CC/1.3.6.1.4.1....   \n",
      "\n",
      "                                    cropped_roi_path  \\\n",
      "0  raw/Mass-Test_P_00066_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
      "1  raw/Calc-Test_P_02176_RIGHT_MLO_1/1.3.6.1.4.1....   \n",
      "2  raw/Calc-Training_P_00418_LEFT_CC_1/1.3.6.1.4....   \n",
      "3  raw/Mass-Test_P_01307_RIGHT_MLO_1/1.3.6.1.4.1....   \n",
      "4  raw/Mass-Training_P_00488_LEFT_CC_1/1.3.6.1.4....   \n",
      "\n",
      "                                       roi_mask_path label  \n",
      "0  raw/Mass-Test_P_00066_LEFT_CC_1/1.3.6.1.4.1.95...   1.0  \n",
      "1  raw/Calc-Test_P_02176_RIGHT_MLO_1/1.3.6.1.4.1....   1.0  \n",
      "2  raw/Calc-Training_P_00418_LEFT_CC_1/1.3.6.1.4....   1.0  \n",
      "3  raw/Mass-Test_P_01307_RIGHT_MLO_1/1.3.6.1.4.1....   0.0  \n",
      "4  raw/Mass-Training_P_00488_LEFT_CC_1/1.3.6.1.4....   0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "paired_metadata_path = \"../data/processed/cbis_ddsm_metadata_paired.csv\"\n",
    "clinical_metadata_path = \"../data/metadata/metadata_master.csv\"\n",
    "output_path = \"../data/processed/cbis_ddsm_metadata_full.csv\"\n",
    "merged_metadata = merge_metadata(paired_metadata_path, clinical_metadata_path, output_path)\n",
    "\n",
    "# Preview\n",
    "print(merged_metadata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5f59f",
   "metadata": {},
   "source": [
    "### Build TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e028b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip uninstall tensorflow -y\n",
    "# python3 -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "# pip install tensorflow==2.15.0\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# pip install pandas pydicom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9902b",
   "metadata": {},
   "source": [
    "#### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1536a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 19:58:59.827654: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 19:58:59.853529: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-10 19:58:59.853558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-10 19:58:59.854478: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-10 19:58:59.859263: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 19:59:00.302700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pydicom\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set TensorFlow logging level to suppress warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Global configuration\n",
    "INPUT_SHAPE = (512, 512, 1)\n",
    "TARGET_SIZE = INPUT_SHAPE[:2]\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce95c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "Num GPUs Available: 2\n",
      "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 19:59:03.208205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:03.208385: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:03.209218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:03.209357: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:03.209477: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:03.209657: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "#  Check TensorFlow version and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c966529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "Current device: 0\n",
      "Device name: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch version and GPU availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f9ac6",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3084f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICOM Loader\n",
    "# Load and normalize a DICOM image from a byte string path\n",
    "def load_dicom_image(path_tensor):\n",
    "    path = path_tensor.decode('utf-8')  # Decode byte string to UTF-8\n",
    "    try:\n",
    "        ds = pydicom.dcmread(path)\n",
    "        img = ds.pixel_array.astype(np.float32)\n",
    "        img -= np.min(img)\n",
    "        img /= (np.max(img) + 1e-6)  # normalize to [0,1]\n",
    "    except Exception as e:\n",
    "        print(f\"[DICOM ERROR] {path}: {e}\")\n",
    "        img = np.zeros(TARGET_SIZE, dtype=np.float32)\n",
    "    return img\n",
    "\n",
    "# TensorFlow Wrappers\n",
    "# Load and preprocess a single full mammogram image\n",
    "def tf_load_dicom(path):\n",
    "    # img = tf.numpy_function(load_dicom_image, [path], tf.float32)\n",
    "    img = tf.numpy_function(func=load_dicom_image, inp=[path], Tout=tf.float32)\n",
    "    img.set_shape([None, None])  # initially 2D\n",
    "    img = tf.expand_dims(img, axis=-1)  # [H, W, 1]\n",
    "    img.set_shape([None, None, 1])\n",
    "    img = tf.image.resize(img, TARGET_SIZE)\n",
    "    return img\n",
    "\n",
    "def tf_load_multiple_dicom(paths):\n",
    "    # paths: tf.Tensor of shape [N] (string paths)\n",
    "    def load_single(path):\n",
    "        img = tf.numpy_function(load_dicom_image, [path], tf.float32)\n",
    "        img.set_shape([None, None])\n",
    "        img = tf.expand_dims(img, axis=-1)\n",
    "        img.set_shape([None, None, 1])\n",
    "        img = tf.image.resize(img, TARGET_SIZE)\n",
    "        return img\n",
    "\n",
    "    masks = tf.map_fn(\n",
    "        load_single,\n",
    "        paths,\n",
    "        fn_output_signature=tf.TensorSpec(shape=(TARGET_SIZE[0], TARGET_SIZE[1], 1), dtype=tf.float32)\n",
    "    )\n",
    "    return tf.reduce_max(masks, axis=0)  # union of all masks\n",
    "\n",
    "# Unified MTL Preprocessor\n",
    "# Load and preprocess multiple ROI masks and combine into a single mask tensor\n",
    "def load_and_preprocess(image_path, mask_paths, label):\n",
    "    image = tf_load_dicom(image_path)  # (512, 512, 1)\n",
    "    mask = tf_load_multiple_dicom(mask_paths)  # (512, 512, 1)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, {\"segmentation\": mask, \"classification\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57e24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a dictionary record into image + MTL target dict\n",
    "def parse_record(record):\n",
    "    image_path = record['image_path']\n",
    "    mask_paths = record['mask_paths']\n",
    "    label = record['label']\n",
    "\n",
    "    image, target = load_and_preprocess(image_path, mask_paths, label)\n",
    "    return image, target\n",
    "\n",
    "# Build tf.data.Dataset from metadata CSV\n",
    "def build_tf_dataset(\n",
    "    metadata_csv: str,\n",
    "    batch_size: int = 8,\n",
    "    shuffle: bool = True\n",
    ") -> tf.data.Dataset:\n",
    "\n",
    "    # Load metadata CSV\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    # Parse stringified list of mask_paths\n",
    "    df['mask_paths'] = df['mask_paths'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "    # Ensure label column is float32-compatible (e.g., 0.0, 1.0)\n",
    "    df['label'] = df['label'].astype(np.float32)\n",
    "\n",
    "    # Convert to list of dicts\n",
    "    records = df[['image_path', 'mask_paths', 'label']].to_dict(orient='records')\n",
    "\n",
    "    # Create dataset\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: (r for r in records),\n",
    "        output_signature={\n",
    "            \"image_path\": tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "            \"mask_paths\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "            \"label\": tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Apply MTL-compatible mapping function\n",
    "    ds = ds.map(lambda r: parse_record(r), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(records))\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22fe0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 19:59:18.500521: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.500694: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.500820: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.500933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.501045: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.501158: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.518988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.519155: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.519281: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.519397: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.519517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.519631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46661 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-05-10 19:59:18.519889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-10 19:59:18.520004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14274 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:03:00.0, compute capability: 8.9\n",
      "2025-05-10 19:59:29.372785: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 113 of 3751\n",
      "2025-05-10 19:59:49.556627: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 514 of 3751\n",
      "2025-05-10 20:00:00.657553: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 715 of 3751\n",
      "2025-05-10 20:00:19.407838: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 1099 of 3751\n",
      "2025-05-10 20:00:29.438236: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 1299 of 3751\n",
      "2025-05-10 20:00:49.251526: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 1651 of 3751\n",
      "2025-05-10 20:00:59.335729: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 1830 of 3751\n",
      "2025-05-10 20:01:19.674989: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 2211 of 3751\n",
      "2025-05-10 20:01:39.516956: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 2572 of 3751\n",
      "2025-05-10 20:01:49.766293: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 2750 of 3751\n",
      "2025-05-10 20:02:09.789302: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 3131 of 3751\n",
      "2025-05-10 20:02:29.733237: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 3478 of 3751\n",
      "2025-05-10 20:02:39.932369: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:3: Filling up shuffle buffer (this may take a while): 3682 of 3751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: (8, 512, 512, 1)\n",
      "Masks batch shape: (8, 512, 512, 1)\n",
      "Labels batch shape: (8,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:02:43.045092: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "# Build dataset\n",
    "train_ds = build_tf_dataset(\n",
    "    metadata_csv=\"../data/processed/cbis_ddsm_metadata_full.csv\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Preview one batch\n",
    "for images, targets in train_ds.take(1):\n",
    "    print(f\"Images batch shape: {images.shape}\")  # (8, 512, 512, 1)\n",
    "    print(f\"Masks batch shape: {targets['segmentation'].shape}\")     # (8, 512, 512, 1)\n",
    "    print(f\"Labels batch shape: {targets['classification'].shape}\")  # (8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df0983",
   "metadata": {},
   "source": [
    "The resulting train_ds is a complete TensorFlow training dataset — (image, {\"segmentation\": mask, \"classification\": label}), normalized, resized, shuffled, batched, ready for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
