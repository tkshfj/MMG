{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513b7c97",
   "metadata": {},
   "source": [
    "# Analyse Weights and Biases Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91594d4c",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries & Load Data\n",
    "# !pip install wandb pandas matplotlib seaborn\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Set display options for wide DataFrames\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e759dd",
   "metadata": {},
   "source": [
    "#### Define project names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define W&B Project names\n",
    "projects = [\n",
    "    \"tkshfj-bsc-computer-science-university-of-london/baseline_cnn_dropout_aug_2\",\n",
    "    \"tkshfj-bsc-computer-science-university-of-london/baseline_cnn_dropout_aug_1\",\n",
    "    \"tkshfj-bsc-computer-science-university-of-london/baseline_cnn_dropout_1\",\n",
    "    \"tkshfj-bsc-computer-science-university-of-london/baseline_shallow_cnn_2\",\n",
    "    \"tkshfj-bsc-computer-science-university-of-london/baseline_shallow_cnn_1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c10dc",
   "metadata": {},
   "source": [
    "#### Connect to W&B and Download Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249325a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "all_runs_hyperparams = []\n",
    "\n",
    "for project in projects:\n",
    "    print(f\"Fetching runs from: {project}\")\n",
    "    runs = api.runs(project)\n",
    "\n",
    "    for run in runs:\n",
    "        if run.state == \"finished\":\n",
    "            config = {k: v for k, v in run.config.items() if not k.startswith(\"_\")}\n",
    "            config['run_id'] = run.id\n",
    "            config['run_name'] = run.name\n",
    "            all_runs_hyperparams.append(config)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_all_hyperparams = pd.DataFrame(all_runs_hyperparams)\n",
    "\n",
    "# Show the result\n",
    "df_all_hyperparams.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_hyperparams.to_csv(\"all_runs_hyperparams.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f1fc2",
   "metadata": {},
   "source": [
    "#### Fetch runs from projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0574863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_runs(project):\n",
    "    runs = api.runs(project)\n",
    "    records = []\n",
    "    for run in runs:\n",
    "        if run.state == \"finished\":\n",
    "            rec = dict(run.summary)\n",
    "            rec.update(dict(run.config))\n",
    "            rec[\"run_id\"] = run.id\n",
    "            rec[\"run_name\"] = run.name\n",
    "            records.append(rec)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60cb80",
   "metadata": {},
   "source": [
    "#### Try to get predictions and labels from run summary or artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_preds_labels(run):\n",
    "    # As summary arrays\n",
    "    preds = run.summary.get('preds') or run.summary.get('y_pred') or run.summary.get('predictions')\n",
    "    labels = run.summary.get('labels') or run.summary.get('y_true') or run.summary.get('ground_truth')\n",
    "    if preds is not None and labels is not None:\n",
    "        # Convert to np arrays if necessary\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "        # If predictions are probabilities, convert to binary\n",
    "        if preds.ndim > 1 and preds.shape[-1] == 2:  # [N, 2]\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        elif preds.ndim == 1 and (preds.max() > 1 or preds.min() < 0):\n",
    "            preds = (preds > 0.5).astype(int)\n",
    "        return preds, labels\n",
    "\n",
    "    # Look for wandb Table artifact\n",
    "    for artifact in run.logged_artifacts():\n",
    "        if \"confusion\" in artifact.name or \"eval\" in artifact.name or \"pred\" in artifact.name:\n",
    "            table = artifact.get(\"table\")\n",
    "            if table:\n",
    "                df = table.dataframe()\n",
    "                pred_col = [c for c in df.columns if \"pred\" in c][0]\n",
    "                label_col = [c for c in df.columns if \"label\" in c][0]\n",
    "                preds = df[pred_col].values\n",
    "                labels = df[label_col].values\n",
    "                return preds, labels\n",
    "    # Not found\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6c8ad",
   "metadata": {},
   "source": [
    "#### Get best run by validation AUC and plot loss and AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "summary_table = []\n",
    "\n",
    "for project in projects:\n",
    "    print(f\"Processing project: {project}\")\n",
    "    df = fetch_runs(project)\n",
    "    \n",
    "    if df.empty or 'epoch/val_auc' not in df.columns:\n",
    "        print(f\"No suitable runs found in {project}\")\n",
    "        continue\n",
    "\n",
    "    # Get best run by validation AUC\n",
    "    best_run_row = df.sort_values('epoch/val_auc', ascending=False).iloc[0]\n",
    "    print(f\"Best run: {best_run_row['run_name']}, val_auc={best_run_row['epoch/val_auc']:.3f}\")\n",
    "\n",
    "    # Append to summary\n",
    "    summary_table.append({\n",
    "        'Project': project.split('/')[-1],\n",
    "        'Run Name': best_run_row['run_name'],\n",
    "        'val_accuracy': best_run_row['epoch/val_accuracy'],\n",
    "        'val_auc': best_run_row.get('epoch/val_auc', None),\n",
    "        'val_loss': best_run_row.get('epoch/val_loss', None)\n",
    "    })\n",
    "\n",
    "    # Plot curves for best run\n",
    "    run = api.run(f\"{project}/{best_run_row['run_id']}\")\n",
    "    history = run.history(keys=[\n",
    "        \"epoch/epoch\", \"epoch/loss\", \"epoch/val_loss\", \"epoch/auc\", \"epoch/val_auc\"\n",
    "    ])\n",
    "\n",
    "    epochs = history[\"epoch/epoch\"]\n",
    "\n",
    "    # Side-by-side plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Loss Curve\n",
    "    axes[0].plot(epochs, history[\"epoch/loss\"], label=\"Train Loss\")\n",
    "    axes[0].plot(epochs, history[\"epoch/val_loss\"], label=\"Val Loss\")\n",
    "    axes[0].set_title(f\"Loss Curve\\n{project.split('/')[-1]} / {best_run_row['run_name']}\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # AUC Curve\n",
    "    axes[1].plot(epochs, history[\"epoch/auc\"], label=\"Train AUC\")\n",
    "    axes[1].plot(epochs, history[\"epoch/val_auc\"], label=\"Val AUC\")\n",
    "    axes[1].set_title(f\"AUC Curve\\n{project.split('/')[-1]} / {best_run_row['run_name']}\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"AUC\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show summary table\n",
    "summary_df = pd.DataFrame(summary_table)\n",
    "print(\"Summary of best runs per project:\")\n",
    "display(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
