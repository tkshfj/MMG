{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972bf625",
   "metadata": {},
   "source": [
    "### Build TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d174e18",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow -y\n",
    "# !python3 -m venv .venv\n",
    "# !source .venv/bin/activate\n",
    "# !pip install tensorflow==2.15.0\n",
    "# !pip install pandas pydicom scikit-learn seaborn nbformat\n",
    "\n",
    "# The cuXXX (CUDA) wheels are Linux-only, NVIDIA GPU-only\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Use the command from the PyTorch website for macOS!\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e3ef8",
   "metadata": {},
   "source": [
    "#### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TensorFlow logging level to suppress warnings\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import ast\n",
    "import pydicom\n",
    "# from tensorflow.data import AUTOTUNE\n",
    "\n",
    "#  Check TensorFlow version and GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Global configuration\n",
    "INPUT_SHAPE = (224, 224, 1)  # (512, 512, 1)\n",
    "TARGET_SIZE = INPUT_SHAPE[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version and GPU availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "# Never use the cuXXX index for macOS!\n",
    "# Use the official PyPI source, or the command from the PyTorch website.\n",
    "# print(\"Current device:\", torch.cuda.current_device())\n",
    "# print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b604807",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114571a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICOM Loader\n",
    "# Load and normalize a DICOM image from a byte string path\n",
    "def load_dicom_image(path_tensor):\n",
    "    path = path_tensor.decode('utf-8')  # Decode byte string to UTF-8\n",
    "    try:\n",
    "        ds = pydicom.dcmread(path)\n",
    "        img = ds.pixel_array.astype(np.float32)\n",
    "        img -= np.min(img)\n",
    "        img /= (np.max(img) + 1e-6)  # normalize to [0,1]\n",
    "    except Exception as e:\n",
    "        print(f\"[DICOM ERROR] {path}: {e}\")\n",
    "        img = np.zeros(TARGET_SIZE, dtype=np.float32)\n",
    "    return img\n",
    "\n",
    "# TensorFlow Wrappers\n",
    "# Load and preprocess a single full mammogram image\n",
    "def tf_load_dicom(path):\n",
    "    # img = tf.numpy_function(load_dicom_image, [path], tf.float32)\n",
    "    img = tf.numpy_function(func=load_dicom_image, inp=[path], Tout=tf.float32)\n",
    "    img.set_shape([None, None])  # initially 2D\n",
    "    img = tf.expand_dims(img, axis=-1)  # [H, W, 1]\n",
    "    img.set_shape([None, None, 1])\n",
    "    img = tf.image.resize(img, TARGET_SIZE)\n",
    "    return img\n",
    "\n",
    "def tf_load_multiple_dicom(paths):\n",
    "    # paths: tf.Tensor of shape [N] (string paths)\n",
    "    def load_single(path):\n",
    "        img = tf.numpy_function(load_dicom_image, [path], tf.float32)\n",
    "        img.set_shape([None, None])\n",
    "        img = tf.expand_dims(img, axis=-1)\n",
    "        img.set_shape([None, None, 1])\n",
    "        img = tf.image.resize(img, TARGET_SIZE)\n",
    "        return img\n",
    "\n",
    "    masks = tf.map_fn(\n",
    "        load_single,\n",
    "        paths,\n",
    "        fn_output_signature=tf.TensorSpec(shape=(TARGET_SIZE[0], TARGET_SIZE[1], 1), dtype=tf.float32)\n",
    "    )\n",
    "    return tf.reduce_max(masks, axis=0)  # union of all masks\n",
    "\n",
    "# Unified MTL Preprocessor\n",
    "# Load and preprocess multiple ROI masks and combine into a single mask tensor\n",
    "def load_and_preprocess(image_path, mask_paths, label):\n",
    "    image = tf_load_dicom(image_path)  # (512, 512, 1)\n",
    "    mask = tf_load_multiple_dicom(mask_paths)  # (512, 512, 1)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, {\"segmentation\": mask, \"classification\": label}\n",
    "\n",
    "# Parse a dictionary record into image + MTL target dict\n",
    "def parse_record(record):\n",
    "    image_path = record['image_path']\n",
    "    mask_paths = record['mask_paths']\n",
    "    label = record['label']\n",
    "\n",
    "    image, target = load_and_preprocess(image_path, mask_paths, label)\n",
    "    return image, target\n",
    "\n",
    "# Build tf.data.Dataset from metadata CSV\n",
    "def build_tf_dataset(\n",
    "    metadata_csv: str,\n",
    "    batch_size: int = 8,\n",
    "    shuffle: bool = True\n",
    ") -> tf.data.Dataset:\n",
    "\n",
    "    # Load metadata CSV\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    # Parse stringified list of mask_paths\n",
    "    df['mask_paths'] = df['mask_paths'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "    # Ensure label column is float32-compatible (e.g., 0.0, 1.0)\n",
    "    df['label'] = df['label'].astype(np.float32)\n",
    "\n",
    "    # Convert to list of dicts\n",
    "    records = df[['image_path', 'mask_paths', 'label']].to_dict(orient='records')\n",
    "\n",
    "    # Create dataset\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: (r for r in records),\n",
    "        output_signature={\n",
    "            \"image_path\": tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "            \"mask_paths\": tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "            \"label\": tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Apply MTL-compatible mapping function\n",
    "    ds = ds.map(lambda r: parse_record(r), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(records))\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172103ee",
   "metadata": {},
   "source": [
    "The resulting ds is a complete TensorFlow dataset — (image, {\"segmentation\": mask, \"classification\": label}), normalized, resized, shuffled, batched, ready for model training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203be548",
   "metadata": {},
   "source": [
    "### Explore the Resulting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "ds = build_tf_dataset(\n",
    "    metadata_csv=\"../data/processed/cbis_ddsm_metadata_full.csv\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Preview one batch\n",
    "for images, targets in ds.take(1):\n",
    "    print(f\"Images batch shape: {images.shape}\")  # (8, 224, 224, 1)\n",
    "    print(f\"Masks batch shape: {targets['segmentation'].shape}\")     # (8, 224, 224, 1)\n",
    "    print(f\"Labels batch shape: {targets['classification'].shape}\")  # (8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for images, targets in ds.take(1):\n",
    "    num_examples = 3  # Number of samples to visualize\n",
    "\n",
    "    plt.figure(figsize=(num_examples * 3, 6))\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        # Plot image\n",
    "        plt.subplot(3, num_examples, i + 1)\n",
    "        plt.imshow(images[i, ..., 0], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "\n",
    "        # Plot mask\n",
    "        plt.subplot(3, num_examples, num_examples + i + 1)\n",
    "        plt.imshow(targets['segmentation'][i, ..., 0], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Mask {i+1}\")\n",
    "\n",
    "        # Display the label\n",
    "        plt.subplot(3, num_examples, 2 * num_examples + i + 1)\n",
    "        label = targets['classification'][i].numpy()\n",
    "        plt.text(0.5, 0.5, str(label), fontsize=16, ha='center', va='center')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Label {i+1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7dd35f",
   "metadata": {},
   "source": [
    "### Develop a Baseline Sequential CNN Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dd6ae",
   "metadata": {},
   "source": [
    "Das et al. (2023) provide a comprehensive overview of the architecture and training process for deep learning-based breast cancer classification, presenting a pipeline that can be adapted to a variety of datasets and tasks. Their framework delineates two primary CNN strategies: **Approach 1 (Shallow CNN)** and **Approach 2 (Deep CNN)**. In this work, we begin by implementing the shallow CNN approach as depicted in their proposed workflow.\n",
    "\n",
    "As a first step, we **develop a baseline convolutional neural network (CNN) model consisting of an encoder and a classification head only**. This model will serve as a foundational benchmark, using a series of convolutional and pooling layers followed by fully connected layers for binary (or multiclass) classification, without any segmentation or auxiliary outputs. Establishing such a baseline is critical for objectively evaluating the impact of subsequent model enhancements.\n",
    "\n",
    "The **“Shallow CNN” approach** in the diagram is a progressive three-part strategy aimed at incrementally increasing model robustness and generalization:\n",
    "- Part 1: CNN with 2 Convolutional Layers\n",
    "- Part 2: CNN with 2 Conv Layers + Dropout\n",
    "- Part 3: CNN with 2 Conv Layers + Data Augmentation\n",
    "\n",
    "For all three parts, the training pipeline begins with data collection (for example, from public datasets such as CBIS-DDSM and INbreast), followed by pre-processing steps including image resizing and partitioning into training and testing sets. The chosen shallow CNN model is then trained on the prepared data, with hyperparameters fine-tuned as needed to achieve the desired accuracy. Performance is evaluated using standard metrics, and the architecture can be iteratively refined based on results.\n",
    "\n",
    "**In summary,** starting with the shallow CNN path and a simple encoder-classifier model enables rapid prototyping and establishes a robust baseline. It also provides valuable insights into the data and the classification task, which can inform the design and tuning of deeper or more complex models in subsequent experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb04ef",
   "metadata": {},
   "source": [
    "### Part 1: CNN with 2 Convolutional Layers\n",
    "  This baseline model consists of two convolutional layers followed by pooling, a dense layer, and an output layer. It serves as the simplest deep learning architecture in the pipeline and acts as a benchmark for evaluating further enhancements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f793f98",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Ensure the models directory exists\n",
    "import os\n",
    "model_dir = os.path.abspath(\"../models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20c5a6",
   "metadata": {},
   "source": [
    "#### Build and compile a shallow CNN model as a baseline\n",
    "\n",
    "Compile the model with an appropriate loss function and optimizer, and train it on the dataset. The model will be evaluated on a validation set to monitor performance metrics such as accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c806d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a shallow CNN model as a baseline\n",
    "def build_shallow_cnn(input_shape=INPUT_SHAPE, num_classes=1):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_shallow_cnn(INPUT_SHAPE)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880553b",
   "metadata": {},
   "source": [
    "#### Split and Build a TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and split\n",
    "metadata = pd.read_csv(\"../data/processed/cbis_ddsm_metadata_full.csv\")\n",
    "train_meta, val_meta = train_test_split(\n",
    "    metadata, test_size=0.2, stratify=metadata['label'], random_state=42\n",
    ")\n",
    "train_meta.to_csv(\"../temporary/train_split.csv\", index=False)\n",
    "val_meta.to_csv(\"../temporary/val_split.csv\", index=False)\n",
    "\n",
    "# Build datasets\n",
    "train_ds = build_tf_dataset(metadata_csv=\"../temporary/train_split.csv\", batch_size=8)\n",
    "val_ds = build_tf_dataset(metadata_csv=\"../temporary/val_split.csv\", batch_size=8)\n",
    "train_ds = train_ds.map(lambda x, y: (x, y[\"classification\"])).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: (x, y[\"classification\"])).prefetch(AUTOTUNE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b30c29",
   "metadata": {},
   "source": [
    "#### Set up Weights & Biases for experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05639eec",
   "metadata": {},
   "source": [
    "- Install Weights & Biases (wandb) for experiment tracking and visualization. \n",
    "  - This tool will help us log metrics, visualize model performance, and manage experiments effectively.\n",
    "```sh\n",
    "pip install wandb\n",
    "```\n",
    "- Login to wandb: \n",
    "    - We need to log in by pasting an API key for the first time we use wandb.\n",
    "```sh\n",
    "wandb login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84da30",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "Train the model using the training dataset, and validate it using the validation dataset. Monitor the training and validation loss and accuracy to ensure the model is learning effectively without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb8a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 68s 59ms/step - loss: 0.5446 - accuracy: 0.7160 - auc: 0.7885 - precision: 0.6901 - recall: 0.5639 - val_loss: 0.6101 - val_accuracy: 0.6711 - val_auc: 0.7251 - val_precision: 0.5951 - val_recall: 0.6278\n",
      "Epoch 5/20\n"
     ]
    }
   ],
   "source": [
    "# Initialize Weights & Biases for experiment tracking\n",
    "WandbMetricsLogger = wandb.keras.WandbMetricsLogger\n",
    "WandbModelCheckpoint = wandb.keras.WandbModelCheckpoint\n",
    "\n",
    "wandb.init(project=\"baseline_part_1_cnn_2_conv_layers\", config={\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 20,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"architecture\": \"shallow_cnn\"\n",
    "})\n",
    "\n",
    "# Train model with callbacks\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        WandbMetricsLogger(),\n",
    "        WandbModelCheckpoint(filepath=\"./models/best_model_epoch.keras\", monitor=\"val_loss\", save_best_only=True),  \n",
    "        # os.path.join(model_dir, \"best_model_epoch{epoch:02d}.keras\"), \n",
    "        EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_loss\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd19fe",
   "metadata": {},
   "source": [
    "#### Integrate wandb in Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b8884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# # Initialize a new run\n",
    "# wandb.init(project=\"your-project-name\")\n",
    "\n",
    "# # Example: log hyperparameters\n",
    "# config = wandb.config\n",
    "# config.learning_rate = 0.001\n",
    "# config.batch_size = 32\n",
    "\n",
    "# # During/after training, log metrics\n",
    "# wandb.log({'accuracy': 0.85, 'loss': 0.3})\n",
    "\n",
    "# # End the run (optional, wandb does this automatically on script exit)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551025d",
   "metadata": {},
   "source": [
    "#### Save history to CSV/Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history to CSV\n",
    "import pandas as pd\n",
    "filename = \"../results/history/baseline-part-1-cnn-2-conv-layers.csv\"\n",
    "# Convert history to DataFrame\n",
    "df = pd.DataFrame(history.history)\n",
    "# Save to CSV\n",
    "df.to_csv(filename, index=True)\n",
    "# Load history from CSV\n",
    "# loaded_history = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"../results/history/baseline-part-1-cnn-2-conv-layers.pkl\"\n",
    "# Save history\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "# To load later\n",
    "# with open(filename, 'rb') as f:\n",
    "#     hist_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1d365",
   "metadata": {},
   "source": [
    "#### Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f892cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'history' is the returned object from model.fit()\n",
    "# Example: history = model.fit(...)\n",
    "\n",
    "# Plot training & validation accuracy, AUC, precision, and recall\n",
    "metrics = ['accuracy', 'auc', 'precision', 'recall']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "for idx, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    plt.plot(history.history[metric], label='Train')\n",
    "    plt.plot(history.history['val_' + metric], label='Validation')\n",
    "    plt.title(metric.capitalize())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5fd77",
   "metadata": {},
   "source": [
    "Loss serves as an indicator of model learning: while a lower loss reflects better model fit, a plateau or persistently high loss signals limited learning capacity. In this case, the shallow model can grasp basic patterns but quickly reaches its performance ceiling. To achieve further reductions in loss—and corresponding gains in accuracy and recall—a deeper architecture or improved data strategies are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62461ec",
   "metadata": {},
   "source": [
    "To understand the behavior and limitations of the shallow CNN model, we visualize performance metrics—including **accuracy**, **AUC (Area Under the ROC Curve)**, **precision**, and **recall**—for both the training and validation datasets across all epochs. By tracking these metrics throughout training, we gain a comprehensive view of the model’s overall performance as well as its ability to correctly identify each class. This approach highlights not only how well the model distinguishes between positive and negative cases but also its strengths and weaknesses in real-world clinical contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a740c7b",
   "metadata": {},
   "source": [
    "Interpretation of the Plots\n",
    "\n",
    "- **Accuracy & AUC:** Both accuracy and AUC increased gradually over time, reflecting the model’s ability to learn basic distinctions between classes. However, validation AUC started near 0.59 and only reached about 0.71 by the end of training (with training AUC slightly higher at 0.82). These values fall short of the 0.8–0.85+ threshold generally expected for clinically useful screening models. Similarly, validation accuracy plateaued around 0.65, which is better than random guessing (0.5 for binary classification) but still not adequate for high-stakes medical applications.\n",
    "\n",
    "- **Precision:** Validation precision showed significant fluctuations, at times exceeding 0.7 but dropping lower in other epochs. This inconsistency points to the model’s variable confidence in positive predictions. In most epochs, precision was higher than recall, indicating the model preferred to make fewer positive predictions—acting cautiously to avoid false positives, but at the expense of missing true positives.\n",
    "\n",
    "- **Recall** was especially unstable, ranging from very low values (as low as 0.07) up to just above 0.6 in certain epochs. This volatility suggests that the model sometimes effectively “gave up” on detecting positives, which could be due to class imbalance or other optimization challenges. Critically, persistently low recall means that the model misses a large proportion of true positive cases (e.g., actual cancers), which is unacceptable in the context of medical diagnosis.\n",
    "\n",
    "- **Summary**: \n",
    "\n",
    "* The shallow CNN is capable of learning some meaningful features, but its performance **saturates quickly**, never advancing beyond moderate accuracy and AUC.\n",
    "* **Recall remains a major weakness:** even at its peak, it fails to reliably identify enough positive cases, severely limiting clinical applicability.\n",
    "* To address these shortcomings, it is recommended to **adopt a deeper, more expressive model architecture** and/or apply **more advanced data processing techniques** such as stronger augmentation, handling class imbalance, or experimenting with alternative loss functions.\n",
    "\n",
    "While the shallow CNN provides a useful learning baseline, substantial improvements are necessary to reach clinically relevant performance, particularly in terms of recall and robust, consistent predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce127ac",
   "metadata": {},
   "source": [
    "### Part 2: CNN with 2 Conv Layers + Dropout\n",
    "  To mitigate overfitting and improve the model’s ability to generalize, a Dropout layer is added after the dense layer. Dropout randomly deactivates a proportion of neurons during training, preventing the network from relying too heavily on specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf95eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_shallow_cnn_dropout(input_shape=INPUT_SHAPE, num_classes=2):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Dropout layer added\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575de12e",
   "metadata": {},
   "source": [
    "### Part 3: CNN with 2 Conv Layers + Data Augmentation\n",
    "  Building on Part 2, data augmentation techniques are introduced during the training phase. Methods such as random rotations, translations, zooms, and horizontal flips are applied to the input images, artificially increasing the diversity of the training set and further reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01b265",
   "metadata": {},
   "source": [
    "We apply data augmentation to your data pipeline to enhance the model's robustness and generalization capabilities. This involves applying transformations such as random rotations, shifts, zooms, and flips to the training images, which helps the model learn invariant features and improves its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af035419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Example of setting up data augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# For validation/test, only rescale\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Example:\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     'data/train',\n",
    "#     target_size=(224, 224),\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',  # or 'binary'\n",
    "#     color_mode='grayscale',    # if images are grayscale\n",
    "# )\n",
    "\n",
    "# Use the same model as Part 2 (with dropout)\n",
    "model = build_shallow_cnn_dropout(input_shape=INPUT_SHAPE, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ba4a8",
   "metadata": {},
   "source": [
    "### Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Use new Keras format for saving\n",
    "checkpoint_cb = ModelCheckpoint(\"../models/best_model.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_loss\")\n",
    "\n",
    "metadata = pd.read_csv(\"../data/processed/cbis_ddsm_metadata_full.csv\")\n",
    "train_meta, val_meta = train_test_split(\n",
    "    metadata, test_size=0.2, stratify=metadata['label'], random_state=42\n",
    ")\n",
    "\n",
    "# Save to new CSV files\n",
    "train_meta.to_csv(\"../temporary/train_split.csv\", index=False)\n",
    "val_meta.to_csv(\"../temporary/val_split.csv\", index=False)\n",
    "\n",
    "# Use your build_tf_dataset as before\n",
    "train_ds = build_tf_dataset(metadata_csv=\"../temporary/train_split.csv\", batch_size=8)\n",
    "val_ds = build_tf_dataset(metadata_csv=\"../temporary/val_split.csv\", batch_size=8)\n",
    "\n",
    "# Keep only the classification label\n",
    "train_ds = train_ds.map(lambda x, y: (x, y[\"classification\"]))\n",
    "val_ds = val_ds.map(lambda x, y: (x, y[\"classification\"]))\n",
    "\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "# Build the model\n",
    "model = build_classification_model(INPUT_SHAPE)\n",
    "\n",
    "# Train the model\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4), \n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Proceed with model.fit\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787f907",
   "metadata": {},
   "source": [
    "### Evaluating and Plotting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    metrics = ['accuracy', 'val_accuracy', 'auc', 'val_auc', 'precision', 'val_precision', 'recall', 'val_recall']\n",
    "    for metric in metrics:\n",
    "        if metric in history.history:\n",
    "            plt.plot(history.history[metric], label=metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.title('Training & Validation Metrics')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39268b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(val_ds)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083933bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect all true labels and predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x_batch, y_batch in val_ds:\n",
    "    # Predict probabilities\n",
    "    probs = model.predict(x_batch)\n",
    "    # For binary, threshold at 0.5\n",
    "    preds = (probs.flatten() > 0.5).astype(int)\n",
    "    y_true.extend(y_batch.numpy().astype(int))\n",
    "    y_pred.extend(preds)\n",
    "    \n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f85a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = [\"BENIGN\", \"MALIGNANT\"]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae274b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = [\"BENIGN\", \"MALIGNANT\"]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "\n",
    "# Annotate values\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
