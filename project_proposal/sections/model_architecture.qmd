---
title: "Model Architecture Roadmap"
---

## Model Architecture Roadmap for Breast Cancer Detection

We adopt a staged modeling framework inspired by Chollet’s *universal deep learning workflow*.[@chollet2021] This project will evolve from foundational convolutional neural networks (CNNs) to advanced transformer-based multi-task architectures for detecting and localizing breast cancer in CBIS-DDSM mammograms.

---

### Stage 1 — Baseline CNN for Binary Classification

The first stage establishes a performance baseline using a simple CNN to classify mammograms as *benign* or *malignant*.

Architecture:

* Input: Grayscale mammogram, resized to a fixed shape (e.g., `(512, 512, 1)`)
* 2–3 convolutional layers with ReLU activations and MaxPooling
* 1–2 dense layers, optionally followed by Dropout
* Output: `Dense(1, activation="sigmoid")`

Training:

* Loss: Binary crossentropy
* Optimizer: Adam
* Metrics: Accuracy, AUC

> Overfitting is expected at this stage, emphasizing the importance of data augmentation and regularization in later stages.

---

### Stage 2 — Enhanced CNNs and U-Net with Transfer Learning

This stage improves classification accuracy and introduces segmentation to localize lesions on mammograms.

#### 2A. Enhanced CNNs for Classification

* Regularization: Dropout, L2 weight decay
* Augmentation: Random flip, rotation, zoom, shift
* Training Strategies: Early stopping, model checkpointing
* Validation: Hold-out or stratified K-fold cross-validation
* Evaluation: Accuracy, ROC-AUC, precision–recall curves, confusion matrix

#### 2B. U-Net for Lesion Segmentation

U-Net enables pixel-wise lesion localization, improving interpretability and model trust.[@ronneberger2015;@el-banby2024]

Architecture:

* Encoder–decoder structure with skip connections
* Input: Grayscale or RGB mammogram
* Output: Binary lesion mask

Variants:

* Vanilla U-Net: Trained from scratch using CBIS-DDSM segmentation masks
* Transfer Learning U-Net:

  * Pretrained encoder (e.g., ResNet34, VGG16)
  * Decoder randomly initialized

Loss Functions:

* Dice coefficient
* Combined binary crossentropy + IoU loss

![U-Net Architecture](/figures/elbandy2024_fig09.png)

---

### Stage 3 — Multi-Task Transformers and Benchmarking

The final stage integrates transfer learning, multi-task learning (MTL), and transformer-based models to approach clinical-grade performance.

#### 3A. Transfer Learning for Classification

* Backbones: EfficientNetB0–B3, ResNet50, InceptionV3
* Input: Mammograms converted to RGB to match ImageNet pretrained weights
* Output: Custom classification head for binary prediction
* Fine-tuning: Lower layers frozen initially, later unfrozen for end-to-end training

#### 3B. Multi-Task Learning (MTL)

Jointly learns classification and segmentation tasks from a shared representation.

Architecture:

* Shared encoder (CNN or Transformer)
* Two output heads:

  * Classification Head: Benign vs. malignant
  * Segmentation Head: Lesion mask

Loss Function:

```
TotalLoss = α * SegmentationLoss + β * ClassificationLoss
```

Advantages:

* Enhanced generalization
* Efficient use of limited data
* Clinically aligned: diagnosis + localization in one model

#### 3C. Transformer-Based Architectures

Modern self-attention architectures improve global context awareness.

* Models: Vision Transformer (ViT), Swin Transformer
* Mechanism: Patch embedding + self-attention layers
* Hybrid Options: TransUNet or ViT-U-Net (CNN encoder + Transformer decoder)

Benefits:

* Better global context capture
* Interpretable via attention heatmaps
* Flexible for both classification and segmentation

#### 3D. Interpretability and Benchmarking

Interpretability Tools:

* Grad-CAM
* Saliency maps
* Attention heatmaps
* Visualization with TensorBoard, Weights & Biases

Benchmarking Strategy:

* Compare model performance across stages
* Classification metrics: Accuracy, AUC, F1-score
* Segmentation metrics: Dice coefficient, IoU

Comparative Works:

* Wang et al. (2024): Deep learning segmentation/detection in mammography

> Optional extensions include ensemble models, attention-guided loss functions, and model calibration techniques.
