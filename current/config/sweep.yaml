# sweep.yaml
program: main.py
method: bayes

metric:
  name: val_multi
  goal: maximize

parameters:
  # Experiment (fixed)
  architecture: {value: multitask_unet}
  task: {value: multitask}
  run_cap: {value: 20}

  # Execution
  epochs: {value: 40}
  num_workers: {value: 30}
  debug: {value: true}
  debug_transforms: {value: false}

  # Data
  in_channels: {value: 1}
  input_shape: {value: [256, 256]}
  num_classes: {value: 2}
  class_counts: {value: [331, 232]}  # [NEG_COUNT, POS_COUNT]
  # pos_weight: {value: 3.33}  

  # Metric weighting (for val_multi)
  multi_weight: {value: 0.65}

  # Tunables
  batch_size: {values: [16, 32, 64]}

  lr:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.003

  weight_decay:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.00012

  dropout_rate:
    distribution: uniform
    min: 0.15
    max: 0.25

  # Loss weighting
  alpha: {values: [0.5, 1.0, 2.0]}
  beta:  {values: [0.5, 1.0, 2.0]}

  # Scheduler strategy
  lr_strategy: {values: ["cosine", "onecycle", "plateau", "warmcos", "none"]}

  # Cosine
  T_max: {value: 40}
  eta_min: {value: 0.0}

  # OneCycle (per-iteration)
  max_lr:
    distribution: log_uniform_values
    min: 0.001
    max: 0.1
  pct_start: {value: 0.3}
  div_factor: {value: 25.0}
  final_div_factor: {value: 10000.0}

  # Plateau (after validation)
  monitor: {values: ["val_loss", "val_auc"]}
  monitor_mode: {values: ["min", "max"]}
  patience: {values: [2, 3, 4, 6]}
  factor: {values: [0.5, 0.3]}

  # Warmup + Cosine
  warmup_epochs: {values: [0, 2, 3, 5]}
  warmup_start_factor: {values: [0.05, 0.1, 0.2]}

# NOTE:
# val_multi gets registered whenever both val_auc and val_dice exist
# get_loss_fn reads alpha, beta
# Optimizer uses cfg.lr and cfg.weight_decay
# Model reads cfg.dropout_rate
# Remove/ignore patch_size for UNet, or gate by architecture
