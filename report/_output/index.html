<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Takeshi Fujii, MD">
<meta name="dcterms.date" content="2025-04-25">

<title>Final Project: Deep Learning for Breast Cancer Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-53f539a478d6ecd1be31ce9fd30fd81c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8a894fcc3cb5e37eb5bf30def131be2f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


<meta name="citation_title" content="Final Project: Deep Learning for Breast Cancer Detection">
<meta name="citation_author" content="Takeshi Fujii, MD">
<meta name="citation_publication_date" content="2025-04-25">
<meta name="citation_cover_date" content="2025-04-25">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-04-25">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Breast cancer detection: Shallow convolutional neural network against deep convolutional neural networks based approach;,citation_abstract=Introduction: Of all the cancers that afflict women, breast cancer (BC) has the second-highest mortality rate, and it is also believed to be the primary cause of the high death rate. Breast cancer is the most common cancer that affects women globally. There are two types of breast tumors: benign (less harmful and unlikely to become breast cancer) and malignant (which are very dangerous and might result in aberrant cells that could result in cancer). Methods: To find breast abnormalities like masses and micro-calcifications, competent and educated radiologists often examine mammographic images. This study focuses on computer-aided diagnosis to help radiologists make more precise diagnoses of breast cancer. This study aims to compare and examine the performance of the proposed shallow convolutional neural network architecture having different specifications against pre-trained deep convolutional neural network architectures trained on mammography images. Mammogram images are pre-processed in this study’s initial attempt to carry out the automatic identification of BC. Thereafter, three different types of shallow convolutional neural networks with representational differences are then fed with the resulting data. In the second method, transfer learning via fine-tuning is used to feed the same collection of images into pre-trained convolutional neural networks VGG19, ResNet50, MobileNet-v2, Inception-v3, Xception, and Inception-ResNet-v2. Results: In our experiment with two datasets, the accuracy for the CBIS-DDSM and INbreast datasets are 80.4%, 89.2%, and 87.8%, 95.1% respectively. Discussion: It can be concluded from the experimental findings that the deep network-based approach with precise tuning outperforms all other state-of-the-art techniques in experiments on both datasets.;,citation_author=Himanish Shekhar Das;,citation_author=Akalpita Das;,citation_author=Anupal Neog;,citation_author=Saurav Mallik;,citation_author=Kangkana Bora;,citation_author=Zhongming Zhao;,citation_publication_date=2023-01;,citation_cover_date=2023-01;,citation_year=2023;,citation_doi=10.3389/fgene.2022.1097207;,citation_issn=1664-8021;,citation_volume=13;,citation_language=en-US;,citation_journal_title=Frontiers in Genetics;">
<meta name="citation_reference" content="citation_title=Automated abnormalities detection in mammography using deep learning;,citation_abstract=Breast cancer is the second most prevalent cause of cancer death and the most common malignancy among women, posing a life-threatening risk. Treatment for breast cancer can be highly effective, with a survival chance of 90% or higher, especially when the disease is detected early. This paper introduces a groundbreaking deep U-Net framework for mammography breast cancer images to perform automatic detection of abnormalities. The objective is to provide segmented images that show areas of tumors more accurately than other deep learning techniques. The proposed framework consists of three steps. The first step is image preprocessing using the Li algorithm to minimize the cross-entropy between the foreground and the background, contrast enhancement using contrast-limited adaptive histogram equalization (CLAHE), normalization, and median filtering. The second step involves data augmentation to mitigate overfitting and underfitting, and the final step is implementing a convolutional encoder-decoder network-based U-Net architecture, characterized by high precision in medical image analysis. The framework has been tested on two comprehensive public datasets, namely INbreast and CBIS-DDSM. Several metrics have been adopted for quantitative performance assessment, including the Dice score, sensitivity, Hausdorff distance, Jaccard coefficient, precision, and F1 score. Quantitative results on the INbreast dataset show an average Dice score of 85.61% and a sensitivity of 81.26%. On the CBIS-DDSM dataset, the average Dice score is 87.98%, and the sensitivity reaches 90.58%. The experimental results ensure earlier and more accurate abnormality detection. Furthermore, the success of the proposed deep learning framework in mammography shows promise for broader applications in medical imaging, potentially revolutionizing various radiological practices.;,citation_author=Ghada M. El-Banby;,citation_author=Nourhan S. Salem;,citation_author=Eman A. Tafweek;,citation_author=Essam N. Abd El-Azziz;,citation_publication_date=2024-10;,citation_cover_date=2024-10;,citation_year=2024;,citation_issue=5;,citation_doi=10.1007/s40747-024-01532-x;,citation_issn=2199-4536, 2198-6053;,citation_volume=10;,citation_language=en-US;,citation_journal_title=Complex &amp;amp;amp; Intelligent Systems;">
<meta name="citation_reference" content="citation_title=World-leading AI trial to tackle breast cancer launched;,citation_author=gov.uk;,citation_publication_date=2025-02;,citation_cover_date=2025-02;,citation_year=2025;,citation_journal_title=Press release;">
<meta name="citation_reference" content="citation_title=Mammogram Pre-processing Using filtering methods for Breast Cancer Diagnosis;,citation_abstract=Cancer is the second most found disease, and Breast cancer is the most common in women. Breast cancer is curable and can reduce mortality, but it needs to be identified early and treated accordingly. Radiologists use different modalities for the identification of Breast cancer. The superiority of Mammograms over other modalities is like minor radiation exposure and can identify different types of cancers. Therefore, mammograms are the most frequently used imaging modality for Breast Cancer Diagnosis. However, noise can be added while capturing the image, affecting the accuracy and analysis of the result. Therefore, using different filtering techniques to pre-process mammograms can enhance images and improve outcomes. For the study, the MIAS dataset has been used. This paper gives a comparative study on filters for Denoising and enhancement of mammograms. The study focuses on filters like Box Filter, Averaging filter, Gaussian Filter, Identical Filter, Convolutional 2D Filter, Median Filter, and Bilateral Filter. Performance measures used to compare these filters are Mean Squared Error (MSE), Structural Similarity Index Measure (SSIM), and Peak Signal-to-noise Ratio (PSNR). All Performance measures are evaluated for all images of MIAS dataset and compared accordingly. Results show that Gaussian Filter, Median Filter, and Bilateral Filter give better results than other filters.;,citation_author=Shah Hemali;,citation_author=Agrawal Smita;,citation_author=Parita Oza;,citation_author=Sudeep Tanwar;,citation_author=Ahmed Alkhayyat;,citation_publication_date=2023-08;,citation_cover_date=2023-08;,citation_year=2023;,citation_issue=4;,citation_doi=10.5815/ijigsp.2023.04.04;,citation_issn=20749074, 20749082;,citation_volume=15;,citation_language=en-US;,citation_journal_title=International Journal of Image, Graphics and Signal Processing;">
<meta name="citation_reference" content="citation_title=Early Breast Cancer Detection Based on Deep Learning: An Ensemble Approach Applied to Mammograms;,citation_abstract=Background: Breast cancer is one of the leading causes of death in women, making early detection through mammography crucial for improving survival rates. However, human interpretation of mammograms is often prone to diagnostic errors. This study addresses the challenge of improving the accuracy of breast cancer detection by leveraging advanced machine learning techniques. Methods: We propose an extended ensemble deep learning model that integrates three state-of-the-art convolutional neural network (CNN) architectures: VGG16, DenseNet121, and InceptionV3. The model utilizes multi-scale feature extraction to enhance the detection of both benign and malignant masses in mammograms. This ensemble approach is evaluated on two benchmark datasets: INbreast and CBIS-DDSM. Results: The proposed ensemble model achieved significant performance improvements. On the INbreast dataset, the ensemble model attained an accuracy of 90.1%, recall of 88.3%, and an F1-score of 89.1%. For the CBIS-DDSM dataset, the model reached 89.5% accuracy and 90.2% specificity. The ensemble method outperformed each individual CNN model, reducing both false positives and false negatives, thereby providing more reliable diagnostic results. Conclusions: The ensemble deep learning model demonstrated strong potential as a decision support tool for radiologists, offering more accurate and earlier detection of breast cancer. By leveraging the complementary strengths of multiple CNN architectures, this approach can improve clinical decision making and enhance the accessibility of high-quality breast cancer screening.;,citation_author=Youness Khourdifi;,citation_author=Alae El Alami;,citation_author=Mounia Zaydi;,citation_author=Yassine Maleh;,citation_author=Omar Er-Remyly;,citation_publication_date=2024-12;,citation_cover_date=2024-12;,citation_year=2024;,citation_issue=4;,citation_doi=10.3390/biomedinformatics4040127;,citation_issn=2673-7426;,citation_volume=4;,citation_language=en-US;,citation_journal_title=BioMedInformatics;">
<meta name="citation_reference" content="citation_title=Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) [Data set]. The Cancer Imaging Archive;,citation_author=Rebecca Sawyer Lee;,citation_author=Francisco Gimenez;,citation_author=Assaf Hoogi;,citation_author=Daniel L Rubin;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_doi=10.7937/K9/TCIA.2016.7O02S9CY;">
<meta name="citation_reference" content="citation_title=A curated mammography data set for use in computer-aided detection and diagnosis research;,citation_abstract=Abstract Published research results are difficult to replicate due to the lack of a standard evaluation data set in the area of decision support systems in mammography; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. This causes an inability to directly compare the performance of methods or to replicate prior results. We seek to resolve this substantial challenge by releasing an updated and standardized version of the Digital Database for Screening Mammography (DDSM) for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography. Our data set, the CBIS-DDSM (Curated Breast Imaging Subset of DDSM), includes decompressed images, data selection and curation by trained mammographers, updated mass segmentation and bounding boxes, and pathologic diagnosis for training data, formatted similarly to modern computer vision data sets. The data set contains 753 calcification cases and 891 mass cases, providing a data-set size capable of analyzing decision support systems in mammography.;,citation_author=Rebecca Sawyer Lee;,citation_author=Francisco Gimenez;,citation_author=Assaf Hoogi;,citation_author=Kanae Kawai Miyake;,citation_author=Mia Gorovoy;,citation_author=Daniel L. Rubin;,citation_publication_date=2017-12;,citation_cover_date=2017-12;,citation_year=2017;,citation_issue=1;,citation_doi=10.1038/sdata.2017.177;,citation_issn=2052-4463;,citation_volume=4;,citation_language=en-US;,citation_journal_title=Scientific Data;">
<meta name="citation_reference" content="citation_title=An open codebase for enhancing transparency in deep learning-based breast cancer diagnosis utilizing CBIS-DDSM data;,citation_author=Ling Liao;,citation_author=Eva M. Aagaard;,citation_publication_date=2024-11;,citation_cover_date=2024-11;,citation_year=2024;,citation_issue=1;,citation_doi=10.1038/s41598-024-78648-0;,citation_issn=2045-2322;,citation_volume=14;,citation_language=en-US;,citation_journal_title=Scientific Reports;">
<meta name="citation_reference" content="citation_title=Maiurilorenzo/CBIS-DDSM-CNN $\cdot$ Hugging Face;,citation_abstract=We’re on a journey to advance and democratize artificial intelligence through open source and open science.;,citation_publisher=https://huggingface.co/maiurilorenzo/CBIS-DDSM-CNN;">
<meta name="citation_reference" content="citation_title=Integrative hybrid deep learning for enhanced breast cancer diagnosis: Leveraging the Wisconsin Breast Cancer Database and the CBIS-DDSM dataset;,citation_author=Patnala S. R. Chandra Murty;,citation_author=Chinta Anuradha;,citation_author=P. Appala Naidu;,citation_author=Deenababu Mandru;,citation_author=Maram Ashok;,citation_author=Athiraja Atheeswaran;,citation_author=Nagalingam Rajeswaran;,citation_author=V. Saravanan;,citation_publication_date=2024-11;,citation_cover_date=2024-11;,citation_year=2024;,citation_issue=1;,citation_doi=10.1038/s41598-024-74305-8;,citation_issn=2045-2322;,citation_volume=14;,citation_language=en-US;,citation_journal_title=Scientific Reports;">
<meta name="citation_reference" content="citation_title=An Effective Ensemble Machine Learning Approach to Classify Breast Cancer Based on Feature Selection and Lesion Segmentation Using Preprocessed Mammograms;,citation_abstract=Background: Breast cancer, behind skin cancer, is the second most frequent malignancy among women, initiated by an unregulated cell division in breast tissues. Although early mammogram screening and treatment result in decreased mortality, differentiating cancer cells from surrounding tissues are often fallible, resulting in fallacious diagnosis. Method: The mammography dataset is used to categorize breast cancer into four classes with low computational complexity, introducing a feature extraction-based approach with machine learning (ML) algorithms. After artefact removal and the preprocessing of the mammograms, the dataset is augmented with seven augmentation techniques. The region of interest (ROI) is extracted by employing several algorithms including a dynamic thresholding method. Sixteen geometrical features are extracted from the ROI while eleven ML algorithms are investigated with these features. Three ensemble models are generated from these ML models employing the stacking method where the first ensemble model is built by stacking ML models with an accuracy of over 90% and the accuracy thresholds for generating the rest of the ensemble models are $&amp;amp;amp;gt;$95% and $&amp;gt;$96. Five feature selection methods with fourteen configurations are applied to notch up the performance. Results: The Random Forest Importance algorithm, with a threshold of 0.045, produces 10 features that acquired the highest performance with 98.05% test accuracy by stacking Random Forest and XGB classifier, having a higher than $>$96% accuracy. Furthermore, with K-fold cross-validation, consistent performance is observed across all K values ranging from 3–30. Moreover, the proposed strategy combining image processing, feature extraction and ML has a proven high accuracy in classifying breast cancer.;,citation_author=A. K. M. Rakibul Haque Rafid;,citation_author=Sami Azam;,citation_author=Sidratul Montaha;,citation_author=Asif Karim;,citation_author=Kayes Uddin Fahim;,citation_author=Md. Zahid Hasan;,citation_publication_date=2022-11;,citation_cover_date=2022-11;,citation_year=2022;,citation_issue=11;,citation_doi=10.3390/biology11111654;,citation_issn=2079-7737;,citation_volume=11;,citation_language=en-US;,citation_journal_title=Biology;">
<meta name="citation_reference" content="citation_title=U-Net: Convolutional Networks for Biomedical Image Segmentation;,citation_abstract=There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .;,citation_author=Olaf Ronneberger;,citation_author=Philipp Fischer;,citation_author=Thomas Brox;,citation_publication_date=2015-05;,citation_cover_date=2015-05;,citation_year=2015;,citation_fulltext_html_url=https://arxiv.org/abs/1505.04597;,citation_doi=10.48550/arXiv.1505.04597;,citation_publisher=arXiv;">
<meta name="citation_reference" content="citation_title=Scolocke/tumor_classification;,citation_author=scolocke;,citation_publication_date=2021-09;,citation_cover_date=2021-09;,citation_year=2021;">
<meta name="citation_reference" content="citation_title=Deep Learning to Improve Breast Cancer Detection on Screening Mammography;,citation_abstract=Abstract The rapid development of deep learning, a family of machine learning techniques, has spurred much interest in its application to medical imaging problems. Here, we develop a deep learning algorithm that can accurately detect breast cancer on screening mammograms using an “end-to-end” training approach that efficiently leverages training datasets with either complete clinical annotation or only the cancer status (label) of the whole image. In this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, eliminating the reliance on rarely available lesion annotations. Our all convolutional network method for classifying screening mammograms attained excellent performance in comparison with previous methods. On an independent test set of digitized film mammograms from the Digital Database for Screening Mammography (CBIS-DDSM), the best single model achieved a per-image AUC of 0.88, and four-model averaging improved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On an independent test set of full-field digital mammography (FFDM) images from the INbreast database, the best single model achieved a per-image AUC of 0.95, and four-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity: 96.1%). We also demonstrate that a whole image classifier trained using our end-to-end approach on the CBIS-DDSM digitized film mammograms can be transferred to INbreast FFDM images using only a subset of the INbreast data for fine-tuning and without further reliance on the availability of lesion annotations. These findings show that automatic deep learning methods can be readily trained to attain high accuracy on heterogeneous mammography platforms, and hold tremendous promise for improving clinical tools to reduce false positive and false negative screening mammography results. Code and model available at: https://github.com/lishen/end2end-all-conv .;,citation_author=Li Shen;,citation_author=Laurie R. Margolies;,citation_author=Joseph H. Rothstein;,citation_author=Eugene Fluder;,citation_author=Russell McBride;,citation_author=Weiva Sieh;,citation_publication_date=2019-08;,citation_cover_date=2019-08;,citation_year=2019;,citation_issue=1;,citation_doi=10.1038/s41598-019-48995-4;,citation_issn=2045-2322;,citation_volume=9;,citation_language=en-US;,citation_journal_title=Scientific Reports;">
<meta name="citation_reference" content="citation_title=Breast Cancer Classification Using Synthesized Deep Learning Model with Metaheuristic Optimization Algorithm;,citation_abstract=Breast cancer is the second leading cause of mortality among women. Early and accurate detection plays a crucial role in lowering its mortality rate. Timely detection and classification of breast cancer enable the most effective treatment. Convolutional neural networks (CNNs) have significantly improved the accuracy of tumor detection and classification in medical imaging compared to traditional methods. This study proposes a comprehensive classification technique for identifying breast cancer, utilizing a synthesized CNN, an enhanced optimization algorithm, and transfer learning. The primary goal is to assist radiologists in rapidly identifying anomalies. To overcome inherent limitations, we modified the Ant Colony Optimization (ACO) technique with opposition-based learning (OBL). The Enhanced Ant Colony Optimization (EACO) methodology was then employed to determine the optimal hyperparameter values for the CNN architecture. Our proposed framework combines the Residual Network-101 (ResNet101) CNN architecture with the EACO algorithm, resulting in a new model dubbed EACO–ResNet101. Experimental analysis was conducted on the MIAS and DDSM (CBIS-DDSM) mammographic datasets. Compared to conventional methods, our proposed model achieved an impressive accuracy of 98.63%, sensitivity of 98.76%, and specificity of 98.89% on the CBIS-DDSM dataset. On the MIAS dataset, the proposed model achieved a classification accuracy of 99.15%, a sensitivity of 97.86%, and a specificity of 98.88%. These results demonstrate the superiority of the proposed EACO–ResNet101 over current methodologies.;,citation_author=Selvakumar Thirumalaisamy;,citation_author=Kamaleshwar Thangavilou;,citation_author=Hariharan Rajadurai;,citation_author=Oumaima Saidani;,citation_author=Nazik Alturki;,citation_author=Sandeep Kumar Mathivanan;,citation_author=Prabhu Jayagopal;,citation_author=Saikat Gochhait;,citation_publication_date=2023-09;,citation_cover_date=2023-09;,citation_year=2023;,citation_issue=18;,citation_doi=10.3390/diagnostics13182925;,citation_issn=2075-4418;,citation_volume=13;,citation_language=en-US;,citation_journal_title=Diagnostics;">
<meta name="citation_reference" content="citation_title=Lazatsoc/cbis_ddsm_dataloader;,citation_author=Lazaros Tsochatzidis;,citation_publication_date=2025-03;,citation_cover_date=2025-03;,citation_year=2025;">
<meta name="citation_reference" content="citation_title=Mammography with deep learning for breast cancer detection;,citation_abstract=X-ray mammography is currently considered the golden standard method for breast cancer screening, however, it has limitations in terms of sensitivity and specificity. With the rapid advancements in deep learning techniques, it is possible to customize mammography for each patient, providing more accurate information for risk assessment, prognosis, and treatment planning. This paper aims to study the recent achievements of deep learning-based mammography for breast cancer detection and classification. This review paper highlights the potential of deep learning-assisted X-ray mammography in improving the accuracy of breast cancer screening. While the potential benefits are clear, it is essential to address the challenges associated with implementing this technology in clinical settings. Future research should focus on refining deep learning algorithms, ensuring data privacy, improving model interpretability, and establishing generalizability to successfully integrate deep learning-assisted mammography into routine breast cancer screening programs. It is hoped that the research findings will assist investigators, engineers, and clinicians in developing more effective breast imaging tools that provide accurate diagnosis, sensitivity, and specificity for breast cancer.;,citation_author=Lulu Wang;,citation_publication_date=2024-02;,citation_cover_date=2024-02;,citation_year=2024;,citation_doi=10.3389/fonc.2024.1281922;,citation_issn=2234-943X;,citation_volume=14;,citation_language=en-US;,citation_journal_title=Frontiers in Oncology;">
<meta name="citation_reference" content="citation_title=Yuyuyu123456/CBIS-DDSM;,citation_publisher=https://github.com/yuyuyu123456/CBIS-DDSM;">
</head>

<body class="quarto-light">

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Final Project: Deep Learning for Breast Cancer Detection</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Takeshi Fujii, MD </p>
          </div>
                      <div class="quarto-title-meta-contents">
            <p class="author"> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        BSc Computer Science, The University of London
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">25 April 2025</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="report.pdf"><i class="bi bi-file-pdf"></i>PDF</a></p></div></div></div>
    </div>



    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">1. Introduction</a>
  <ul class="collapse">
  <li><a href="#background-motivation" id="toc-background-motivation" class="nav-link" data-scroll-target="#background-motivation">1.1 Background &amp; Motivation</a></li>
  <li><a href="#objectives" id="toc-objectives" class="nav-link" data-scroll-target="#objectives">1.2 Objectives</a></li>
  <li><a href="#scope" id="toc-scope" class="nav-link" data-scroll-target="#scope">1.3 Scope</a></li>
  </ul></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">2. Dataset</a>
  <ul class="collapse">
  <li><a href="#dataset-description" id="toc-dataset-description" class="nav-link" data-scroll-target="#dataset-description">2.1 Dataset Description</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">2.2 Preprocessing</a></li>
  <li><a href="#splitting-strategy" id="toc-splitting-strategy" class="nav-link" data-scroll-target="#splitting-strategy">2.3 Splitting Strategy</a></li>
  </ul></li>
  <li><a href="#deep-learning-workflow" id="toc-deep-learning-workflow" class="nav-link" data-scroll-target="#deep-learning-workflow">3. Deep Learning Workflow</a>
  <ul class="collapse">
  <li><a href="#problem-definition" id="toc-problem-definition" class="nav-link" data-scroll-target="#problem-definition">3.1 Problem Definition</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">3.2 Data Preparation</a></li>
  <li><a href="#model-building" id="toc-model-building" class="nav-link" data-scroll-target="#model-building">3.3 Model Building</a></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training">3.4 Model Training</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation">3.5 Evaluation</a></li>
  <li><a href="#model-improvement" id="toc-model-improvement" class="nav-link" data-scroll-target="#model-improvement">3.6 Model Improvement</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">4. Results</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">5. Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">6. Conclusion</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#a.-code-snippets" id="toc-a.-code-snippets" class="nav-link" data-scroll-target="#a.-code-snippets">A. Code Snippets</a></li>
  <li><a href="#b.-hyperparameter-table" id="toc-b.-hyperparameter-table" class="nav-link" data-scroll-target="#b.-hyperparameter-table">B. Hyperparameter Table</a></li>
  <li><a href="#c.-full-model-architecture" id="toc-c.-full-model-architecture" class="nav-link" data-scroll-target="#c.-full-model-architecture">C. Full Model Architecture</a></li>
  <li><a href="#d.-data-statistics" id="toc-d.-data-statistics" class="nav-link" data-scroll-target="#d.-data-statistics">D. Data Statistics</a></li>
  <li><a href="#e.-report-writing-tools" id="toc-e.-report-writing-tools" class="nav-link" data-scroll-target="#e.-report-writing-tools">E. Report Writing Tools</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<ul>
<li>One-paragraph summary of the problem, dataset, methodology, and main findings.</li>
<li>One-paragraph summary of the problem, dataset, approach, and key results.</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<section id="background-motivation" class="level3">
<h3 class="anchored" data-anchor-id="background-motivation">1.1 Background &amp; Motivation</h3>
<ul>
<li>Describe the clinical and societal significance of early breast cancer detection.</li>
<li>Mention the NHS 2025 initiative and how AI fits into screening.</li>
</ul>
<p><img src="figures/mammogram.jpg" class="center img-fluid" style="width:50.0%" alt="Screening mammogram"> <!-- \begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/mammogram.jpg}
\caption{Breast cancer screening mammogram. AI enhances diagnostic accuracy.}
\end{figure} --></p>
<p>According to recent research<span class="citation" data-cites="lee2017"><sup><a href="#ref-lee2017" role="doc-biblioref">1</a></sup></span>, neural networks outperform …</p>
</section>
<section id="objectives" class="level3">
<h3 class="anchored" data-anchor-id="objectives">1.2 Objectives</h3>
<ul>
<li>Apply deep learning (CNNs) to mammography image classification.</li>
<li>Evaluate performance vs.&nbsp;traditional methods/radiologists.</li>
</ul>
</section>
<section id="scope" class="level3">
<h3 class="anchored" data-anchor-id="scope">1.3 Scope</h3>
<p>Briefly note focus on classification (benign vs malignant), dataset used, and evaluation metrics.</p>
<p>Background &amp; Motivation</p>
<ul>
<li>Significance of early breast cancer detection.</li>
<li>NHS 2025 initiative on DL for screening.</li>
</ul>
<p>Project Objectives</p>
<ul>
<li>Build and evaluate CNN models using the DDSM/CBIS-DDSM dataset.</li>
<li>Assess whether CNNs can match or exceed radiologist performance.</li>
</ul>
<p>Scope</p>
<ul>
<li>Focus on classification (benign vs.&nbsp;malignant), with optional segmentation.</li>
<li>Use curated public data for transparency and reproducibility.</li>
</ul>
</section>
</section>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">2. Dataset</h2>
<section id="dataset-description" class="level3">
<h3 class="anchored" data-anchor-id="dataset-description">2.1 Dataset Description</h3>
<ul>
<li>Dataset: CBIS-DDSM</li>
<li>Number of cases: 753 calcifications, 891 masses</li>
<li>Modalities: Mammograms with labels and ROI masks</li>
</ul>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">2.2 Preprocessing</h3>
<ul>
<li>Resizing, normalization, augmentation</li>
<li>ROI extraction (if applied)</li>
</ul>
</section>
<section id="splitting-strategy" class="level3">
<h3 class="anchored" data-anchor-id="splitting-strategy">2.3 Splitting Strategy</h3>
<ul>
<li>Training, validation, and test set proportions</li>
<li>Use of predefined splits if applicable</li>
</ul>
<p>Dataset Description</p>
<ul>
<li>Use of CBIS-DDSM<span class="citation" data-cites="lee2016 lee2017"><sup><a href="#ref-lee2017" role="doc-biblioref">1</a>,<a href="#ref-lee2016" role="doc-biblioref">2</a></sup></span> — curated version of DDSM.</li>
<li>Number of images, classes (benign/malignant), calcifications vs.&nbsp;masses.</li>
</ul>
<p>Preprocessing Steps</p>
<ul>
<li>ROI extraction, resizing, normalization.</li>
<li>Augmentation techniques (flipping, rotation, etc.).</li>
</ul>
<p>Train/Validation/Test Split</p>
<ul>
<li>Based on BI-RADS or predefined splits from the dataset.</li>
</ul>
</section>
</section>
<section id="deep-learning-workflow" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-workflow">3. Deep Learning Workflow</h2>
<section id="problem-definition" class="level3">
<h3 class="anchored" data-anchor-id="problem-definition">3.1 Problem Definition</h3>
<p>Define input/output: - Input: X-ray mammogram or ROI - Output: Binary label (benign or malignant)</p>
<p>Define the supervised classification task: - Input: X-ray mammogram image (ROI or full view) - Output: Binary label (benign/malignant)</p>
</section>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">3.2 Data Preparation</h3>
<ul>
<li>Preprocessing steps
<ul>
<li>Denoising, rescaling, grayscale conversion</li>
<li>Normalization [e.g., pixel range 0–1 or mean/std]</li>
</ul></li>
<li>Label encoding</li>
<li>Data augmentation techniques: flips, rotations, zooms</li>
</ul>
</section>
<section id="model-building" class="level3">
<h3 class="anchored" data-anchor-id="model-building">3.3 Model Building</h3>
<ul>
<li>Baseline model: custom CNN</li>
<li>Advanced models:
<ul>
<li>Transfer learning (e.g., VGG16, ResNet50)</li>
<li>Optional segmentation with U-Net</li>
</ul></li>
</ul>
</section>
<section id="model-training" class="level3">
<h3 class="anchored" data-anchor-id="model-training">3.4 Model Training</h3>
<ul>
<li>Loss function: Binary Crossentropy</li>
<li>Optimizer: Adam</li>
<li>Metrics: Accuracy, AUC, Sensitivity, Specificity, F1-score</li>
<li>Epochs, batch size, learning rate, early stopping, callbacks (e.g., early stopping, LR scheduler)</li>
</ul>
</section>
<section id="evaluation" class="level3">
<h3 class="anchored" data-anchor-id="evaluation">3.5 Evaluation</h3>
<ul>
<li>Report performance on test set
<ul>
<li>Confusion matrix</li>
<li>ROC curve, AUC</li>
<li>Precision, Recall, F1-score</li>
</ul></li>
</ul>
</section>
<section id="model-improvement" class="level3">
<h3 class="anchored" data-anchor-id="model-improvement">3.6 Model Improvement</h3>
<ul>
<li><p>Regularization techniques: dropout, L2</p></li>
<li><p>Data augmentation experiments</p></li>
<li><p>Architecture tuning: more layers, batch norm<br>
</p></li>
<li><p>Transfer learning comparisons</p></li>
<li><p>Add dropout / L2 regularization</p></li>
<li><p>Increase network depth</p></li>
<li><p>Apply transfer learning</p></li>
<li><p>Tune hyperparameters</p></li>
</ul>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">4. Results</h2>
<ul>
<li>Performance Tables: Accuracy, AUC, Sensitivity, Specificity per model</li>
<li>Visualizations: ROC curve, training/validation loss curves</li>
<li>Error Analysis: Misclassified cases, confusion matrix</li>
<li>Example visualizations of predictions (e.g., Grad-CAM)</li>
</ul>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">5. Discussion</h2>
<ul>
<li>Compare results with literature benchmarks
<ul>
<li>Comparison with Radiologists (Wang 2024)</li>
</ul></li>
<li>Strengths and limitations of the model/approach</li>
<li>Interpretability &amp; practical deployment considerations
<ul>
<li>Grad-CAM (optional)</li>
</ul></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">6. Conclusion</h2>
<ul>
<li>Summary of findings</li>
<li>Implications for clinical use: Whether deep learning improves screening performance</li>
<li>Suggestions for future work: Recommendations for future research (ensemble models, multi-task learning)</li>
</ul>
<!-- ## 7. References
{{< include /sections/_references.qmd >}} -->
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<section id="a.-code-snippets" class="level3">
<h3 class="anchored" data-anchor-id="a.-code-snippets">A. Code Snippets</h3>
<ul>
<li>Add code snippets here later</li>
</ul>
</section>
<section id="b.-hyperparameter-table" class="level3">
<h3 class="anchored" data-anchor-id="b.-hyperparameter-table">B. Hyperparameter Table</h3>
<ul>
<li>Add hyperparameter tables</li>
</ul>
</section>
<section id="c.-full-model-architecture" class="level3">
<h3 class="anchored" data-anchor-id="c.-full-model-architecture">C. Full Model Architecture</h3>
<ul>
<li>Add full model architecture</li>
</ul>
</section>
<section id="d.-data-statistics" class="level3">
<h3 class="anchored" data-anchor-id="d.-data-statistics">D. Data Statistics</h3>
<ul>
<li>Add any dataset distribution histograms or BI-RADS label breakdowns</li>
</ul>
</section>
<section id="e.-report-writing-tools" class="level3">
<h3 class="anchored" data-anchor-id="e.-report-writing-tools">E. Report Writing Tools</h3>
<p>The writing process for this report was conducted using <strong>Quarto</strong>, a modern scientific and technical publishing system that integrates <strong>Markdown</strong>, <strong>LaTeX</strong>, and executable code within a single framework. The project uses the <code>manuscript</code> type configuration to generate both <strong>PDF (via XeLaTeX)</strong> and <strong>HTML outputs</strong> with consistent styling, numbered sections, and title-cased tables of contents. The directory follows a modular structure (<code>_quarto.yml</code>, <code>report.qmd</code>), with customizations for fonts, TOC titles, and citation formatting via <code>.bib</code> and <code>.csl</code> files. <strong>Version control</strong> was managed using <strong>Git and GitHub</strong>, enabling reproducible and collaborative manuscript development. Integrated with <strong>VSCode</strong> and <strong>Zotero (via Better BibTeX)</strong>, this setup provides a complete academic writing workflow—featuring live previews, citation support, and source-controlled outputs—crucial for high-quality, reproducible scientific communication.</p>
</section>
</section>
<section id="references" class="level2 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-lee2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Lee, R. S. <em>et al.</em> <a href="https://doi.org/10.1038/sdata.2017.177">A curated mammography data set for use in computer-aided detection and diagnosis research</a>. <em>Scientific Data</em> <strong>4</strong>, 170177 (2017).</div>
</div>
<div id="ref-lee2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Lee, R. S., Gimenez, F., Hoogi, A. &amp; Rubin, D. L. Curated <span>Breast Imaging Subset</span> of <span>Digital Database</span> for <span>Screening Mammography</span> (<span>CBIS-DDSM</span>) [<span>Data</span> set]. <span>The Cancer Imaging Archive</span>. (2016) doi:<a href="https://doi.org/10.7937/K9/TCIA.2016.7O02S9CY">10.7937/K9/TCIA.2016.7O02S9CY</a>.</div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>