---
title: "Literature Review: Deep Learning for Breast Cancer Detection"
author:
  - name: "Takeshi Fujii, MD"
    affiliation: "BSc Computer Science, The University of London"
date: today
date-format: "D MMMM YYYY"
bibliography: ../references/references.bib
csl: ../references/styles/harvard.csl
link-citations: true
format:
  pdf:
    documentclass: scrartcl
    classoption: ["DIV=11", "numbers=noendperiod"]
    header-includes:
      - "\\KOMAoption{captions}{tableheading}"
---

# Introduction

<!-- * Brief overview of the project topic.
* Importance of the problem domain.
* Aim of the literature review.
* Overview of what will be covered (e.g., datasets, models, techniques, gaps). -->

Early detection of breast cancer through mammography significantly improves treatment outcomes. Recent advances in deep learning have shown promising performance in automating this task, particularly with convolutional neural networks (CNNs) and vision transformers (ViTs). This literature review surveys key datasets, architectures, and methods relevant to our project, highlighting current challenges and research gaps.

# Datasets and Benchmarks

<!-- * Introduce key datasets used in the field (e.g., CBIS-DDSM, INbreast).
* Mention how they are structured and annotated.
* Discuss strengths and limitations. -->

## CBIS-DDSM

Lee et al. (2017) curated the CBIS-DDSM dataset, an enhanced version of the original DDSM archive, which provides segmented regions of interest and pathology-confirmed labels. This dataset is widely used for training and benchmarking deep learning models in mammography research [@lee2017].

## INbreast

The INbreast dataset offers full-field digital mammograms with detailed BI-RADS annotations. While limited in size, it is useful for validation and fine-tuning models trained on larger datasets.

# Deep Learning Architectures

<!-- * Review key algorithms (e.g., CNNs, ViTs, MTL models).
* Discuss results and challenges.
* Compare different architectures if relevant. -->

## Convolutional Neural Networks (CNNs)

<!-- * Describe techniques like transfer learning, data augmentation, explainability.
* Justify their use in your project. -->

Rafid et al. (2022) used a modified VGG-16 to classify benign and malignant lesions, achieving strong performance with Grad-CAM visualizations [@rafid2022]. However, overfitting remains a concern due to dataset imbalance.

## Vision Transformers (ViTs)

Kumar and Saha (2024) applied a transformer-based model to classify breast cancer lesions, demonstrating competitive accuracy with reduced reliance on large convolutional filters [@kumarsaha2024].

# Multi-task Learning and Explainability

<!-- * Summarize tools such as Grad-CAM, LIME, SHAP.
* Discuss relevance for healthcare and interpretability. -->

Murty et al. (2024) proposed a multi-task learning (MTL) framework to jointly perform classification and segmentation, improving robustness [@murty2024]. For interpretability, techniques like Grad-CAM and SHAP are commonly employed [@khourdifi2024].

# Comparative Analysis

<!-- * Highlight common findings across sources.
* Identify gaps in current research.
* Explain how your project addresses these gaps or builds upon existing work. -->

While CNNs dominate the field, transformer-based approaches are emerging, especially for capturing global context. Datasets like CBIS-DDSM enable standardization, but domain shift and lack of diversity remain challenges. Multi-task frameworks show potential for clinically relevant outcomes.

# Conclusion

<!-- * Recap the key takeaways from the literature.
* Reinforce how the reviewed literature informs or justifies your project. -->

This review outlines foundational resources and techniques in mammographic cancer detection using deep learning. Our project builds upon these insights by implementing an interpretable, multi-task transformer model trained on curated datasets.

# References

<!-- * Use the University of London referencing style.
* Include all cited materials (books, papers, datasets, tools, websites).
* Ensure proper formatting and completeness. -->

# Optional Elements

<!-- * Tables: For comparing models, datasets, or performance metrics.
* Figures: Architecture diagrams, Grad-CAM examples, data examples.
* Appendices: If needed for extended figures or supporting details. -->
